{"pages":[],"posts":[{"title":"Http 协议","date":"2017-12-08T11:04:00.000Z","path":"计算机网络/http-协议/","text":"Http 历史版本HTTP 是基于 TCP/IP 协议的应用层协议。主要规定了客户端和服务器之间的通信格式，默认使用80端口。 Http0.91991年发布，只有 GET，只能回应 html 格式的字符串。 请求示例：1GET /index.html 响应示例：123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; Http1.01996年5月发布，记载于 RFC1945；可以传输任何类型的内容（文本、图像、视频、二进制文件等） 相对比http0.9增加了： POST、HEAD 等请求方法 状态码 MIME 首部字段 权限认证（Basic） 字符集 传输各类型内容 详细内容请参考 RFC1945 请求示例：123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */* 响应示例：12345678910HTTP/1.0 200 OKContent-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 缺点:每个TCP连接只能发送一个请求。 Http1.11997年1月发布，最初版是 RFC2068；1999年6月发布的修订版 RFC2616 沿用至今。 对比http1.0最大的变化是： 默认持久连接（keep-alive） 引入了 pipelining（同一个TCP连接发送多个请求），但是由于服务器是按照请求顺序进行响应，客户端无法确定最优请求顺序，所以未普及 分块传输编码(chunked)，针对大数据传输操作，把数据切分为若干数据块[1]进行传输（流模式） 增加了更多的请求方法：OPTIONS,PUT,DELETE,TRACE,CONNECT 增加了更多的状态码 可以范围取数据,可实现断点续传下载(range,etag) 可追踪数据请求链(via) 增加权限认证 增加了host首部，使得一台服务器可以支持虚拟主机 缺点： 浏览器对同一域名下的持久连接并发有限制，通常是6个 使用pipelining会造成 Head of line blocking 每次通信都需要传送header，多次请求时大部分请求首部字段不变，会增加传输成本 URL / URI / URN定义uniform resource identifier（统一资源标识符） URIuniform resource locator （统一资源定位符）URLuniform resource name（统一资源名称）URN 区别URL、URN 是 URI 的子集 URI可被视为定位符（URL），名称（URN）或两者兼备。统一资源名（URN）如同一个人的名称，而统一资源定位符（URL）代表一个人的住址。换言之，URN定义某事物的身份，而URL提供查找该事物的方法。 Http 报文 通用首部字段 请求首部字段 响应首部字段 实体首部字段 Http 状态码 1XX 状态码 含义 100 Continue 101 Switching Protocols 2XX 状态码 含义 说明 200 OK 201 Created 202 Accepted 203 Non-Authoritative Information 204 No Content 响应报文中不含主体 205 Reset Content 206 Parial Content 范围请求；响应报文中含有content-range 3XX 状态码 含义 说明 300 Multiple Choices 301 Moved Permanently 永久重定向，丢失搜索引擎权重 302 Found 临时重定向 303 See other 有另外的URL，应使用GET去访问另一个URL 304 Not Modified 没有满足客户端的条件请求的响应需要返回，不包含主体 305 Use Proxy 306 (Unuesed) 307 Temporary Redirect 临时重定向，不会把POST改为GET 4XX 状态码 含义 说明 400 Bad Request 请求报文存在语法错误 401 Unauthorized HTTP认证未通过 402 Payment Required 403 Forbidden 请求被拒绝 404 Not Found 未找到请求资源 405 Method NOt Allowed 406 Not Acceptable 407 Proxy Authentication Required 408 Request Timeout 409 Conflict 410 Gone 请求资源被永久性删除 411 Length Required 412 Precondition Failed 413 Request Entity Too Large 414 Request-URI Too Long 415 Unsupported Media Type 416 Requested Range Not Satisfiable 417 Expectation Failed - 5XX 状态码 含义 说明 500 Internal Server Error 服务端在执行请求时发生错误 501 Not Implemented 502 Bad Gateway 503 Service Unavailable 服务器超负载，暂时无法处理请求 504 Gateway Timeout 505 Http Version Not Unsupported - HTTP 追加协议SPDY 协议Google在2010年发布了 SPDY协议；希望在协议层面解决http的一些痛点： 一条连接发送一条请求 请求只能从客户端发起 首部未经压缩 SPDY协议的主要功能有： 多路复用流（单一TCP连接） 请求优先级 压缩Http首部 支持服务端推送数据到客户端 服务器主动提示客户端所需资源（过期） WebSocket 协议Websocket即web浏览器与web服务器之间全双工通信标准。2011年12月11日记载于 RFC6455； 为了实现Websocket通信，在HTTP连接建立之后，需要完成一次握手： Websocket的功能： 服务器推送 减少通信量（websocket首部信息很小） js可调用 Websocket API HTTPS2000年5月公布，发布于 RFC2818；默认端口443。因为有加密解密，所以访问会比http慢（可接受），并且会消耗服务器cpu资源。 Http的缺点 通信使用明文，可能被窃听 不验证通信方身份，可能被伪装 无法证明报文的完整性，可能被篡改 Https的组成 Http + 加密 + 认证 + 完整性保护 = Https Http直接和TCP通信，Https先和SSL/TLS通信，再由SSL/TLS和TCP通信。 加密采用「对称加密」[2]和「非对称加密」[3]混合加密方式；交换密钥使用「非对称加密」，通信使用「对称加密」。 认证采用数字认证机构与其相关机关颁发的「公开密钥证书」进行认证。 网站提交公钥给数字认证机构，机构制作为「公开密钥证书」（非对称加密） 网站服务端把「公开密钥证书」发送给客户端，客户端利用浏览器内置的数字认证机构公钥进行验证（非对称加密） 验证通过后，客户端和服务端使用公钥进行加密通信（对称加密） 完整性保护在通信过程中，应用层会发送MAC（Message Authentication Code）报文摘要；MAC可以查知报文是否被篡改。 HTTP22015年5月，发布于 RFC7540；基于spdy3.0草案；Http2 可以基于「明文」，也可以基于「TLS」进行通信。 关于后续版本：HTTP2后取消了小版本，后续的版本会是HTTP3。 HTTP2的特性1. 二进制协议 length 定义了整个frame（帧）的大小 type定义frame的类型（一共10种） flags用bit位定义一些重要的参数 stream id用作流控制 frame payload就是request的正文。 2. 多路复用的流 帧：HTTP2 数据通信的最小单位；例如请求和响应等，消息由一个或多个帧组成。 流：存在于连接中的一个虚拟通道。流可以承载双向消息，每个流都有一个唯一的整数ID。 HTTP2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流（并发）。流的多路复用意味着在同一连接中来自各个流的数据包会被混合在一起，Stream Identifier将连接上传输的每个帧都关联到一个“流”。 3. 流的优先级和依赖性优先级：每个流都包含一个优先级（也就是“权重”），它被用来告诉对端哪个流更重要。依赖性：借助于PRIORITY帧，客户端可以告知服务器当前的流依赖于其他哪个流。 4. header压缩HTTP2 对消息头采用 HPACK（专为http/2头部设计的压缩格式）进行压缩传输，能够节省消息头占用的网络的流量。 HTTP2 对这些首部采取了压缩策略： HTTP2 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键－值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在HTTP2 的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键－值对要么被追加到当前表的末尾，要么替换表中之前的值。 5. 重置消息Http2 通过发送RST_STREAM帧可终止当前传输的消息并重新发送一个新的。 6. 服务器推送这个功能通常被称作“缓存推送”。主要的思想是：当一个客户端请求资源X，而服务器知道它很可能也需要资源Z的情况下，服务器可以在客户端发送请求前，主动将资源Z推送给客户端。这个功能帮助客户端将Z放进缓存以备将来之需。 服务器推送需要客户端显式的允许服务器提供该功能。但即使如此，客户端依然能自主选择是否需要中断该推送的流。如果不需要的话，客户端可以通过发送一个RST_STREAM帧来中止。 7. 流量控制类似TCP协议通过sliding window的算法来做流量控制，http2 使用 WINDOW_UPDATE frame 来做流量控制。每个stream都有流量控制，这保证了数据接收方可以只让自己需要的数据被传输。 只有数据帧会受到流量控制。 参考资料 《图解 HTTP》 Http2协议 HTTP2简介和基于HTTP2的Web优化 1.每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。 ↩2.加密和解密使用同一个密钥；消耗资源少且快 ↩3.使用公钥进行加密（发送方），使用私钥进行解密（使用方）；消耗资源多且慢 ↩","tags":[],"categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/categories/计算机网络/"}]},{"title":"常见错误","date":"2017-11-18T10:36:00.000Z","path":"数据库/Mysql/常见错误/","text":"ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction原因：select @@autocommit;+————–+| @@autocommit |+————–+| 0 |+————–+没有开启自动提交，导致update一直未commit。 解决： 查看是否有慢查SQL 查看innodb的事务表INNODB_TRX是否有正在锁定的事务线程,有的话看看ID是否在show full processlist里面的sleep线程中，如果是，就证明这个sleep的线程事务一直没有commit或者rollback而是卡住了，我们需要手动kill掉 开启自动提交: set global autocommit=1;","tags":[],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"常用函数","date":"2017-11-17T17:32:00.000Z","path":"数据库/Mysql/常用函数/","text":"进制转换函数CONV(N,from_base,to_base) N是要转换的数据，from_base是原进制，to_base是目标进制。 字符串函数substring(column,pos,length)截取字符串.pos开始的位置，length截取的长度left(‘string’,length)取左边length个字符right(‘string’,length)取右边length个字符 排序函数field(column,’b’,’a’,’c’)显式的排序，按照bac排序 条件函数USING(id) 等于 on(a.id=b.id) //join on条件的缩写 时间函数 unix_timestamp(‘2013-04-01’) from_unixtime(‘1364745600’) ip转换函数 inet_aton(‘192.168.1.1’) inet_ntoa(‘3232235777 ‘)","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"常用SQL","date":"2017-11-17T17:20:00.000Z","path":"数据库/Mysql/常用Sql/","text":"批量更新gist：multi_update.php 在线动态抓取SQL1tcpdump -i eth0 -s 0 -l -w - dst port 3306|strings 随机一条1234SELECT t1.*FROM `table` AS t1 JOIN (SELECT ROUND(RAND() * (SELECT MAX(id) FROM `table`)) AS id) AS t2WHERE t1.id &gt;= t2.idORDER BY t1.id ASC LIMIT 1; 导出数据字典gist：generator_mysql_dict.php 分页SQL采用内连接（INNER JOIN）实现，更高效 1234select cols from tables inner join ( select pk from tables where col1 = $col1 order by col2 limit 1000000,10) using (pk); 检查数据表大小 进去指定 schema 数据库（存放了其他的数据库的信息） 12mysql&gt; use information_schema;Database changed 查询所有数据的大小 12345678mysql&gt; select concat(round(sum(DATA_LENGTH/1024/1024), 2), 'MB')-&gt; as data from TABLES;+-----------+| data |+-----------+| 6674.48MB |+-----------+1 row in set (16.81 sec) 查看指定数据库实例的大小，比如说数据库 forexpert 12345678mysql&gt; select concat(round(sum(DATA_LENGTH/1024/1024), 2), 'MB')-&gt; as data from TABLES where table_schema='forexpert';+-----------+| data |+-----------+| 6542.30MB |+-----------+1 row in set (7.47 sec) 查看指定数据库的表的大小，比如说数据库 forexpert 中的 member 表 123456789mysql&gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),'MB') as data-&gt; from TABLES where table_schema='forexpert'-&gt; and table_name='member';+--------+| data |+--------+| 2.52MB |+--------+1 row in set (1.88 sec) 使用 OR 时,使用 union all 替换1select * from table where id = 1 union all select * from table where id =2; 导入导出数据导入 常规使用 values 导入字段顺序必须一致 123mysql&gt;source ~/dump.sql或mysql -u root -p database_name &lt; ~/dump.sql 使用 load data infile方式一：直接导入 1234567891011121314#!/bin/bash#$1,数据文件的绝对地址username='root'password=''host='127.0.0.1'database=\"thirdsite_grab\"table=\"resumes_contacts\"field=\"src,src_no,resume_updated_at,name,phone,tel,email,create_at,updated_at,is_deleted,status,icdc_id,error_msg,source,type\"if [ -f \"$1\" ]thenmysql -u $username -p$passwoed -h $host $database -e \"load data local infile '$1' replace into table $table ($field)\"elseecho $1'不存在'fi 方式二：不更新索引，加快导入速度 执行 FLUSH TABLES 语句或命令 mysqladmin flush-tables 使用myisamchk –keys-used=0 -rq /path/to/db/tbl_name。这将从表中取消所有索引的使用 用LOAD DATA INFILE把数据插入到表中 用myisamchk -r -q /path/to/db/tbl_name重新创建索引.这将在写入磁盘前在内存中创建索引树，并且它更快，因为避免了大量磁盘搜索。结果索引树也被完美地平衡 执行FLUSH TABLES语句或mysqladmin flush-tables命令。 更简洁的方式二：使用这种方式，不需要执行FLUSH TABLES。 使用ALTER TABLE tbl_name DISABLE KEYS代替myisamchk –keys-used=0 -rq/path/to/db/tbl_name， 使用ALTER TABLE tbl_name ENABLE KEYS代替myisamchk -r -q/path/to/db/tbl_name。 导出 导出 sql 1mysqldump -uxxx -pxxx -h127.0.0.1 database_name table_name -t --where=\"where条件\" &gt; dump.sql 导出文本文件 12345678910111213141516171819#!/bin/bashusername='root'password=''host='127.0.0.1'database=\"thirdsite_grab\"table=\"resumes_contacts\"if [ -n \"$1\" ]thensave_path=$1elsesave_path=/tmp/export.datafiif [ -f \"$save_path\" ]thenecho '删除旧文件'`rm $save_path`fiecho '保存路径为'$save_pathmysql -u $user -p$password $database -h $host -Ne \"set names UTF8;select $field from $table\" &gt; $save_path 删除重复的数据demo：表名；id：自增 ID；site：具有相同数据的列 只有 crud 权限 删除 ID 大的数据 1delete from a using demo as a, demo as b where (a.id &gt; b.id) and (a.site = b.site); 删除 ID 小的数据 1delete from a using demo as a, demo as b where (a.id &lt; b.id) and (a.site = b.site); 有索引权限1234//自动删除重复数据（id 大的删除）alter ignore table demo add unique index ukey (site);//删除刚建立的索引alter table demo drop index ukey;","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"运维指南","date":"2017-11-17T16:37:00.000Z","path":"数据库/Mysql/运维指南/","text":"系统与软件系统 一般选择RHEL、CentOS 拒绝使用32位系统，升级到64位 不追新，稳定、高性能压倒一切 版本一致，批量部署，管理方便 /tmp使用/dev/shm的tmpfs 内核 IO调度：deadline，noop，反正不要cfq VM管理：vm.swappiness=0 文件系统:xfs/zfs 全B+树，高效 分配组，提高并发度 延迟分配，减少IO mount：nobarrier、data=ordered,writeback Mysql使用内存盘（挂载）将 MYSQL 目录迁移到 4G 的内存磁盘12345mkdir -p /mnt/ramdisksudo mount -t tmpfs -o size=4000M tmpfs /mnt/ramdisk/mv /var/lib/mysql /mnt/ramdisk/mysqlln -s /tmp/ramdisk/mysql /var/lib/mysqlchown mysql:mysql mysql 是否关闭swap？mysql 专用服务器则关闭 swap；否则保证innodb_buffer_pool_size足够大 内存使用考量 理论上，内存越大，越多数据读取发生在内存，效率越高 Query cache的使用 如果前端请求重复度不高，或者应用层已经充分缓存重复请求，query cache不必设置很大，甚至可以不设置。 如果前端请求重复度较高，无应用层缓存，query cache是一个很好的偷懒选择 对于中等以下规模数据库应用，偷懒不是一个坏选择。 如果确认使用query cache，记得定时清理碎片，flush query cache. 要考虑到现实的硬件资源和瓶颈分布 学会理解热点数据，并将热点数据尽可能内存化 所谓热点数据，就是最多被访问的数据。 通常数据库访问是不平均的，少数数据被频繁读写，而更多数据鲜有读写。 学会制定不同的热点数据规则，并测算指标。 热点数据规模，理论上，热点数据越少越好，这样可以更好的满足业务的增长趋势。 响应满足度，对响应的满足率越高越好。 比如依据最后更新时间，总访问量，回访次数等指标定义热点数据，并测算不同定义模式下的热点数据规模 系统优化vm.swappinessrhel6及以下设置为0 ；rhel7以上设置为10（0 可能会被 oom kill 掉） /sys/block/sdX/queue/scheduler 内核默认的 cfq 很烂，不要使用普通的 sas 盘建议使用 deadline策略ssd 使用 noop 或者 deadline（还可以设置 io scheduler ） 文件系统 首选xfs，其次ext4，zfs也很不错，但在linux下不是那么可靠(relserfs也不错，但作者被抓，没人维护，不建议使用)高 io 下使用 xfs，不要使用 ext4（会有瓶颈）因为：ext4下%util 基本到了100%，再也上不去了；而 xfs 还有上升的空间 文件系统数据结构xfs 目录内容 b+tree，文件分配b+treeext4 -&gt; htree（特殊 b 树）-&gt; extents/bitmapext3 -&gt; htree -&gt; bitmap 查看现在文件系统的命令df -HT 参数配置老叶的 my.cnf 生成器 myisam 参数MyISAM存储引擎有一个系统变量concurrent_insert，专门用以控制其并发插入的行为，其值分别可以为0、1或2。当concurrent_insert设置为0时，不允许并发插入。当concurrent_insert设置为1时，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是MySQL的默认设置。当concurrent_insert设置为2时，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录。 MyISAM的锁调度 MyISAM存储引擎的读锁和写锁是互斥的，读写操作是串行的。那么，一个进程请求某个MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前！这是因为MySQL认为写请求一般比读请求要重要。这也正是MyISAM表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！幸好我们可以通过一些设置来调节MyISAM的调度行为。 通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。 通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低。 通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。 关注的参数全局参数 interactive_timeout/wait_timeout断开活跃连接超时的时间/断开不活跃连接的超时时间前者数值基本上继承与后者;建议两者设置一样.1.有连接池，timeout 调大一些，或者默认值2.无连接池，建议设置300以内（理论是越小越好，真正的值根据业务b） open_files_limit此选项不够时的错误：can’t open file : \\test\\…..(error:24)sh&gt; perror 24 #查看错误24的含义OS error code 24: too many open files如何调整？ 调整此参数的限制 ulimit -n 修改内核级别的限制 max_connections连接数不够用，建议业务方进行优化，保证连接数可用，不是一味的调大连接数有可能会导致系统雪崩/oom_killer建议 临时调高数量，让业务优化sql 调低 timeout 的值 和 open_files_limit类似的问题问题：Can not connect to Mysql Server1135: Can&#39;t create a new thread(errno 12);if you are not out of available memory, you can consult the manual for a possible OS-dependent bug 解决： 12345vi /etc/security/limits.d/90-nproc.confnproc 改为 65536 或者vi /etc/bashrculimit -u 65536 thread_pool官方版本不支持此功能（线程池）；可以使用 percona,mariadb 分支版本http://imysql.com/2014/07/02/percona-thread-pool-benchmark-testing.shtml线程池作用：让线程分组去处理，而不是阻塞等待处理 内存参数 tmp_table_size/max_heap_table_size不要把 tmp_table_size 、max_heap_table_size（这个是会话级别） 设置过大，建议不高于100M innodb参数 innodb_buffer_pool建议设置50-70%，缓存大量的脏数据，事务信息，锁信息等；保证不会 oom 的前提下，可以设置80%或更高；非专业人士，不建议设置过高。设置太小的话： tps 很低，大量等待 可能会 table full 锁不够用 innodb_data_file_path默认只有10m，ibdata1初始化时，至少设置1G参考： http://imysql.com/2010/06/01/mysql-faq-what-contains-with-ibdata1.htmlInnodb 共享表空间文件 ibdata1 中存储了以下几部分信息： Data dictionary Double write buffer Insert buffer Rollback segments UNDO space 因此在初始化ibdata1时，最好设置大一些，这样可以避免高并发下导致ibdata1急剧增大，大大影响性能 innodb_flush_log_at_trx_commit0\\1\\2\\每秒\\每事务\\0和1的折中（刷新 log入磁盘） transaction_isolation事务隔离级别：默认是 RR（可重复读），建议使用 RR不建议使用 RC，RC并发提高了，锁等待很严重，tpmc 并没提高多少 innodb_log_file_size &amp; innodb_log_buffer_size若1分钟产生10m事务，则 log_buffer 设置64m；100m事务则设置512m；一般不建议超过512m；若发生 innodb_log_buffer_wait_free事件，则需要调整不易设置过大，因为若 mysql 挂掉，重启时会读 log恢复；越大恢复越慢 innodb_file_per_table=1为每个表分别创建 InnoDB FILE；防止ibdata1 文件过大 innodb_buffer_pool_size将数据完全保存在innodb_buffer_pool_size（内存）中 如何确定取值？mysql&gt; SHOW GLOBAL STATUS LIKE &apos;innodb_buffer_pool_pages_%&apos;; //Innodb_buffer_pool_pages_free = 0 则说明需要增加 //innodb_additional_mem_pool_size = 1/200 of buffer_pool //innodb_max_dirty_pages_pct 80% innodb_log_file_size推荐innodb_log_file_size 设置为 0.25 * innodb_buffer_pool_size如果用 1G 的 innodb_log_file_size ，假如服务器宕机，需要 10 分钟来恢复。 innodb_flush_log_at_trx_commit这个选项和写磁盘操作密切相关： innodb_flush_log_at_trx_commit = 1 则每次修改写入磁盘 innodb_flush_log_at_trx_commit = 0/2 每秒写入磁盘 如果你的应用不涉及很高的安全性 (金融系统)，或者基础架构足够安全，或者 事务都很小，都可以用 0 或者 2 来降低磁盘操作。 innodb_flush_method=O_DIRECT避免双写入缓冲 其他 general_log通常不打开，否则会使文件很大，会带来mysql 事务性能下降因为： 记录全部 general log 时 tpmc 大约是不打开 log 时的73.28%，而记录全部 slow log 时的 tpmc 大约是不打开 log 时的59.53%http://imysql.com/2014/09/01/mysql-faq-impact-of-generallog.shtml innodb_max_dirty_pages_pct log_bin一定要打开，数据丢失可找回；影响：性能下降46%，启用赋值或要求数据灾难时损失最小，必须开启该功能，这时考虑 log_bin与数据隔离或者使用 io 能力强设备，可提高响应，密集写很高的应用要启用 ssd 设备，来缓解该类压力binlog 是顺序写；可用 raid0或1来专门存 binlog.主从结构时，从的 bin_log 也打开 long_query_time不要设置为0 （0的作用：每个 sql 记录和 general_log 的作用相当）；可以设置为0.01，0.1等其他值 sync_binlog需要很高的数据一致性，设置为1（每个 sql 都要刷入 binlog）；不特别需要时，可设置为2，3，10等更高；不需要时设为0 log_slow_query分析慢查询用 table_open_cache key_buffer_size query_cache_size memlock 数据预热如何预热？基于 innodb_buffer_pool 的备份和加载；因为数据只有在执行一次，才能加载到innodb_buffer_pool 硬件原则 性能差不多，关键是可靠性 上线前烤机测试非常重要 监控预警可有效预防故障 避免使用外部阵列 最好是2U机型，并且配备RAID卡(with BBU) 安装 所有磁盘组建大阵列，不降低IOPS 默认阵列级别为：raid 1+0 结合业务特征设置主机名，唯一命名 合理利用hosts/dns，可用于应用授权管理 master和slave命名区分开 /tmp使用/dev/shm &amp; tmpfs 部署基本工具包：sysstat、oprofile等 BIOS 设置优化 System Profile（系统配置）选择Performance Per Watt Optimized(DAPC)，发挥最大功耗性能，充分利用 cpu Memory Frequency（内存频率）选择Maximum Performance（最佳性能） C1E，允许在处理器处于闲置状态时启用或禁用处理器切换至最低性能状态，建议关闭 （默认启用） C States（C状态），允许启用或禁用处理器在所有可用电源状态下运行，建议关闭（默 认启用） IO 子系统优化 专业的阵列卡配备CACHE（大部分是1G，4G 很少）及BBU模块（给缓存提供后备电量），提高IOPS cache 策略设置写策略为WB（wirte back），或者FORCE WB，禁用WT策略WB: 让数据先写入 cache，再写回磁盘 阵列卡配置关闭预读，没必要预读，那点宝贵的CACHE用来做写缓存 阵列级别使用RAID 1+0，而不是RAID 5 (1+0的ios写性能比5高) 关闭物理磁盘cache策略（非阵列卡的 cache），防止断电时丢数据 使用高转速硬盘，不使用低转速盘最低1w 转，最好是1.5w 使用SSD或者PCIe-SSD盘ssd性能比 sata 硬盘提升几百倍pcle-ssd 提升上万倍 监控、安全与备份监控 监控与数据分析是一切优化的基础。 没有运营数据监测就不要妄谈优化！ 监控要注意不要产生太多额外的负载，不要因监控带来太多额外系统开销 监控软件nagios、zabbix、cacti 监控体系系统监控服务器资源监控 Cpu, 内存，硬盘空间，i/o压力 设置阈值报警 服务器流量监控 外网流量，内网流量 设置阈值报警 连接状态监控 Show processlist 设置阈值，每分钟监测，超过阈值记录 应用监控慢查询监控 慢查询日志 如果存在多台数据库服务器，应有汇总查阅机制。 请求错误监控 高频繁应用中，会出现偶发性数据库连接错误或执行错误，将错误信息记录到日志，查看每日的比例变化。 偶发性错误，如果数量极少，可以不用处理，但是需时常监控其趋势。 会存在恶意输入内容，输入边界限定缺乏导致执行出错，需基于此防止恶意入侵探测行为。 微慢查询监控 高并发环境里，超过0.01秒的查询请求都应该关注一下。 频繁度监控 写操作，基于binlog，定期分析。 读操作，在前端db封装代码中增加抽样日志，并输出执行时间。 分析请求频繁度是开发架构 进一步优化的基础 最好的优化就是减少请求次数！ 安全数据安全 重点：先可用性而后才是性能 关闭公网，只留私网 密码足够长度、复杂度 开启iptables策略 只开放必要的授权许可 使用普通账号管理mysqld(结合sudo) 集成定期安全检查到监控系统中 性能与安全性考量数据提交方式 innodb_flush_log_at_trx_commit = 1 每次自动提交，安全性高，i/o压力大 innodb_flush_log_at_trx_commit = 2 每秒自动提交，安全性略有影响，i/o承载强。 日志同步 Sync-binlog =1 每条自动更新，安全性高，i/o压力大 Sync-binlog = 0 根据缓存设置情况自动更新，存在丢失数据和同步延迟风险，i/o承载力强。 个人建议保存binlog日志文件，便于追溯 更新操作和系统恢复。 如对日志文件的i/o压力有担心，在内存宽裕的情况下，可考虑将binlog 写入到诸如 /dev/shm 这样的内存映射分区，并定时将旧有的binlog转移到物理硬盘。 性能与安全本身存在相悖的情况，需要在业务诉求层面决定取舍 学会区分什么场合侧重性能，什么场合侧重安全 学会将不同安全等级的数据库用不同策略管理 备份备份恢复 利用slave执行备份 定期全备+及时增备 不定期随机做恢复测试 二进制内容备份使用 –hex-blob 备份方式：mysqldump VS XtraBackup 如何快速备份/恢复？(并发？快照？) 如何执行在线热备1mysqldump --single-transaction或Xtrabackup 高可用架构 Keepalived + LVS Heartbeat + LVS Master + Slave 多Master共享存储 故障转移处理要点 程序与数据库的连接，基于虚地址而非真实ip，由负载均衡系统监控。 保持主从结构的简单化，否则很难做到故障点摘除。 思考方式 遍历对服务器集群的任何一台服务器，前端web，中间件，监控，缓存，db等等，假设该服务器出现故障，系统是否会出现异常？用户访问是否会出现异常。 目标：任意一台服务器崩溃，负载和数据操作均会很短时间内自动转移到其他服务器，不会影响业务的正常进行。不会造成恶性的数据丢失。（哪些是可以丢失的，哪些是不能丢失的） 常见故障 复制报错：主键冲突 硬件、系统崩溃：数据页损坏 误操作：数据误删除 硬件故障：阵列卡(掉线、IO性能下降)、CPU、内存 压测基准测试tpcc、sysbench现在测试建议使用 tpcc，不使用 sysbench（因为有一定的局限性） 压力测试mysqlslap、前端加压 存储/写入压力优化 顺序读写性能远高于随机读写 将顺序写数据和随机读写数据分成不同的物理磁盘进行，有助于i/o压力的疏解 数据库文件涉及索引等内容，写入是随即写 binlog文件是顺序写 淘宝数据库存储优化是这样处理的 部分安全要求不高的写入操作可以用 /dev/shm 分区存储，简单变成内存写。 多块物理硬盘做raid10，可以提升写入能力 关键存储设备优化，善于比对不同存储介质的压力测试数据。 例如fusion-io在新浪和淘宝都有较多使用。 涉及必须存储较为庞大的数据量时 压缩存储，可以通过增加cpu开销（压缩算法）减少i/o压力。前提是你确认cpu相对空闲而i/o压力很大。 新浪微博就是压缩存储的典范。 通过md5去重存储，案例是QQ的文件共享，以及dropbox这样的共享服务，如果你上传的是一个别人已有的文件，计算md5后，直接通过md5定位到原有文件，这样可以极大减少存储量。涉及文件共享，头像共享，相册等应用，通过这种方法可以减少超过70%的存储规模，对硬件资源的节省是相当巨大的。缺点是，删除文件需要甄别该md5是否有其他人使用。 去重存储，用户量越多，上传文件越多，效率越高！ 文件尽量不要存储到数据库内。尽量使用独立的文件系统存储，该话题不展开。 工具集合调优工具 systemtap sar gdb gcore oprofile pmp (Poor Man’s Profiler) dstat 其他工具 Xtrabackup备份出来的数据，可以导入另外一个库，只要制定配置文件就行 ioprofile pt-online-schema-change pt-table-checksum pt-query-digest + Box Anemometer/Query-Digest-UI – 分析 slow logindex_ratio 是 digest 的一个指标，意思是：总共扫描的记录数+最终返回的结果记录数 mysqldumpslow – 分析 slow log pt-ioprofile – 查看哪个数据库读写频繁 开发人员行为 批量导入、导出数据须提前通知DBA，请求协助观察 推广活动或上线新功能须提前通知DBA，请求压力评估 不使用SUPER权限连接数据库 单表多次ALTER操作必须合并为一次操作 数据库DDL及重要SQL及早提交DBA评审 重要业务库须告知DBA重要等级、数据备份及时性要求 不在业务高峰期批量更新、查询数据库 提交线上DDL需求，所有SQL语句须有备注说明 开发环境搭建 启用log_queries_not_using_indexes 设置long_query_time为最小值 定期检查分析slow log 授权和生产环境一致 关闭Query Cache 设置较小InnoDB Buffer Pool、key buffer size 数据量不能太少，否则有些性能问题无法提前规避 误区 分配内存越多越好，可能导致OS Swap专用服务器，禁用 swap; 考虑 numa 也禁 session级内存分配过大，导致OOM 索引越多越好，可能导致更多IO Qcache设置过大，实际效果差(建议关闭) 认为MyISAM的只读效率远高于InnoDB(不一定) 过度优化，反而带来成本的上升（改业务逻辑，改 sql，买昂贵的 io 设备，但实际上多加几级 cache 就可以解决）","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"性能优化","date":"2017-11-17T14:20:00.000Z","path":"数据库/Mysql/性能优化/","text":"优化步骤 第一步，完成数据库查询的优化，需要理解索引结构，才能学会判断影响结果集。而影响结果集对查询效率线性相关，掌握这一点，编写数据查询语句就很容易判断系统开销，了解业务压力趋势。 第二步，在SQL语句已经足够优化的基础上，学会对数据库整体状况的分析，能够对异常和负载的波动有正确的认识和解读；能够对系统资源的分配和瓶颈有正确的认识。学会通过监控和数据来进行系统的评估和优化方案设计，杜绝拍脑袋，学会抓大放小，把握要点的处理方法。 第三步，在彻底掌握数据库语句优化和运维优化的基础上，学会分布式架构设计，掌握复杂，大容量数据库系统的搭建方法。 优化SQL建立慢查询优化系统利用 pt-query-digest 定期分析slow query log，并结合 Box Anemometer 构建slow query log分析及优化系统。 SELECT只选择自己需要的列，不要盲目的使用* ，因为不需要的列可能会导致内存 buffer pool 被这些“无效”数据把真正的热点数据给洗出去了（尤其有 text/blob 列时） JOIN要把过滤性最大（不一定是数据量最小哦，而是只加了WHERE条件后过滤性最大的那个）的表选为驱动表。此外，如果JOIN之后有排序，排序字段一定要属于驱动表，才能利用驱动表上的索引完成排序。 排序绝大多数情况下，排序的代价通常要来的更高，因此如果看到执行计划中有 Using filesort，优先创建排序索引吧。 EXPLAIN详情见：mysql5.6 explain 结果解析： 字段名 值 说明 type ALL 表示预计会进行全表扫描（full table scan）。通常全表扫描的代价是比较大的，建议创建适当的索引，通过索引检索避免全表扫描。此外，全索引扫描（full index scan）的代价有时候是比全表扫描还要高的，除非是基于InnoDB表的主键索引扫描。 Extra Using temporary 表示需要创建临时表以满足需求，通常是因为GROUP BY的列没有索引，或者GROUP BY和ORDER BY的列不一样，也需要创建临时表，建议添加适当的索引。 Using filesort 表示无法利用索引完成排序，也有可能是因为多表连接时，排序字段不是驱动表中的字段，因此也没办法利用索引完成排序，建议添加适当的索引。 Using where 通常是因为全表扫描或全索引扫描时（type 列显示为 ALL 或 index），又加上了WHERE条件，建议添加适当的索引。 查询/优化影响结果集查询 通过 Explain 分析 SQL，查看 rows 列内容 通过慢查询日志的Rows_examined: 后面的数字 优化核心：减少影响结果集数目 基于影响结果集的理解去优化，不论从数据结构，代码，还是涉及产品策略上，都需要贯彻下去。 涉及 limit $start,$num的搜索，如果$start巨大，则影响结果集巨大，搜索效率会非常难过低，尽量用其他方式改写为 limit 0,$num； 确系无法改写的情况下，先从索引结构中获得 limit $start,$num 或limit $start,1 ；再用in操作或基于索引序的 limit 0,$num 二次搜索。 避免所有锁表的情况（比如 delete 操作无索引时） 案例实时排行榜背景： 用户提交游戏积分，显示实时排名。 原方案：设置积分区间，每个区间增加数据统计字段；每次都只 count 本区间的名次，再加上前面所有区间的总和。 提交积分是插入记录，略， select count(*) from jifen where gameid=$gameid and fenshu&gt;$fenshu 问题与挑战：即便索引是 gameid+fenshu 复合索引，涉及count操作，当分数较低时，影响结果集巨大，查询效率缓慢，高峰期会导致连接过多。 优化思路：SQL 优化减少影响结果集，又要取得实时数据，单纯从SQL上考虑，不太有方法。 方案1：逻辑优化 将游戏积分预定义分成数个积分断点，然后分成积分区间，原始状态，每个区间设置一个统计数字项，初始为0。 每次积分提交时，先确定该分数属于哪两个区间之间，这个操作非常简单，因为区间是预定义的，而且数量很少，只需遍历即可，找到最该分数符合的区间，该区间的统计数字项（独立字段，可用内存处理，异步回写数据库或文件）+1。 记录该区间上边界数字为$duandian。SQL: select count(*) from jifen where gameid=$gameid and fenshu&gt;$fenshu and fenshu&lt;$duandian，如果处于第一区间，则无需$duandian，这样因为第一区间本身也是最好的成绩，影响结果集不会很多。 通过该SQL获得其在该区间的名次。获取前面区间的总数总和。（该数字是直接从上述提到的区间统计数字获取，不需要进行count操作）将区间内名次+前区间的统计数字和，获得总名次。 该方法关键在于，积分区间需要合理定义，保证积分提交成绩能平均散落在不同区间。（如涉及较多其他条件，如日排行，总排行，以及其他独立用户去重等，请按照影响结果集思路自行发挥。） 方案2：Redis方案Zset 有序数组结构，分两个子结构，第一是多层树形的存储结构，第二是每个树形节点的计数器，这样类似于前面的分段方式，可以理解为多层分段方式，所以查询效率更高，缺点是更新效率有所增加。 大数据量翻页背景：常见论坛帖子页 SQL: select * from post where tagid=$tagid order by lastpost limit $start, $end 翻页 。索引为 tagid+lastpost 复合索引 挑战：超级热帖，几万回帖，用户频频翻到末页，limit 25770,30 一个操作下来，影响结果集巨大(25770+30)，查询缓慢。 方案1：点击页码改为上下翻页；跳转页sql改为子查询或者内连接 只涉及上下翻页情况每次查询的时候将该页查询结果中最大的 $lastpost和最小的分别记录为 $minlastpost 和 $maxlastpost ，上翻页查询为 select from post where tagid=$tagid and lastpost&lt;$minlastpost order by lastpost desc limit 30; 下翻页为 select from post where tagid=$tagid and lastpost&gt;$maxlastpost order by lastpost limit 30; 使用这种方式，影响结果集只有30条，效率极大提升。 涉及跳转到任意页互联网上常见的一个优化方案可以这样表述，select from post where tagid=$tagid and lastpost&gt;=(select lastpost from post where tagid=$tagid order by lastpost limit $start,1) order by lastpost limit 30; 或者 select from post where pid in (select pid from post where tagid=$tagid order by lastpost limit $start,30); (第2条S语法在新的mysql版本已经不支持，新版本mysql in的子语句不再支持limit条件，但可以分解为两条SQL实现，原理不变，不做赘述) 以上思路在于，子查询的影响结果集仍然是$start +30，但是数据获取的过程（Sending data状态）发生在索引文件中，而不是数据表文件，这样所需要的系统开销就比前一种普通的查询低一个数量级，而主查询的影响结果集只有30条，几乎无开销。但是切记，这里仍然涉及了太多的影响结果集操作。 常见杀手级SQL SELECT * vs SELECT col1, col2减少磁盘 io 和网络 io ORDER BY RAND()http://imysql.com/2014/07/04/mysql-optimization-case-rand-optimize.shtml LIMIT huge_num, offsethttp://imysql.com/2015/04/02/mysql-optimization-case-discuz-hot-post-very-old-paging-optimizing.shtml SELECT COUNT(*) on InnoDB table WHERE func(key_col) = ? – 无法使用索引 WHERE key_part2 =? AND key_part3 =? – 无法使用索引 WHERE key_part1 &gt; ? AND key_part2 =? – 只能用到部分索引 SELECT … WHERE key_col + ? = ? – 无法使用索引 开启/分析慢查询开启检查是否开启 1show variables like '%slow%'; //0为未开启 开启慢查询123456//my.cnf 添加[mysqld]log_slow_queries = 日志保存位置long_query_time=1 #超过一秒的查询保存到日志log-queries-not-using-indexes #没有使用到索引的查询保存在日志log-slow-admin-statements #一些管理指令，也会被记录。比如OPTIMEZE TABLE, ALTER TABLE等等。 分析 mysqldumpslow mysql自带,参数可–help查看 1234# -s：排序方式。c , t , l , r 表示记录次数、时间、查询时间的多少、返回的记录数排序；# ac , at , al , ar 表示相应的倒序；# -t：返回前面多少条的数据；# -g：包含什么，大小写不敏感的； 使用案例： 12mysqldumpslow -s r -t 10 /slowquery.log #slow记录最多的10个语句mysqldumpslow -s t -t 10 -g \"left join\" /slowquery.log #按照时间排序前10中含有\"left join\"的 mysqlsla github地址: 点此进入 使用案例： 123// mysqlsla会自动判断日志类型，为了方便可以建立一个配置文件“~/.mysqlsla”// 在文件里写上：top=100，这样会打印出前100条结果。mysqlsla /data/mysqldata/slow.log 结果说明 1234567891011121314* queries total: 总查询次数* unique:去重后的sql数量* sorted by : 输出报表的内容排序最重大的慢sql统计信息, 包括平均执行时间, 等待锁时间, 结果行的总数, 扫描的行总数.* Count, sql的执行次数及占总的slow log数量的百分比.* Time, 执行时间, 包括总时间, 平均时间, 最小, 最大时间, 时间占到总慢sql时间的百分比.* 95% of Time, 去除最快和最慢的sql, 覆盖率占95%的sql的执行时间.* Lock Time, 等待锁的时间.* 95% of Lock , 95%的慢sql等待锁时间.* Rows sent, 结果行统计数量, 包括平均, 最小, 最大数量.* Rows examined, 扫描的行数量.* Database, 属于哪个数据库* Users, 哪个用户,IP, 占到所有用户执行的sql百分比* Query abstract, 抽象后的sql语句* Query sample, sql语句 优化索引 定期检查并删除重复的索引用 pt-duplicate-key-checker 工具比如 index idx1(a, b) 索引已经涵盖了 index idx2(a)，就可以删除 idx2 索引了。 索引量基于索引的条件过滤，如果优化器意识到总共需要扫描的数据量超过 30% 时，就会直接改变执行计划为全表扫描，不再使用索引。 联合索引WHERE中过滤条件的字段顺序和索引 无需一致排序、分组则就 必须一致 合理利用覆盖索引，但字段尽量不超过5个 合理利用最左索引（前缀索引/部分索引） 及时删除冗余索引 索引字段条件不使用函数 内存表(HEAP 表)使用HASH索引时，不能使用范围检索或者ORDER BY等，只能使用=或者!= 两个独立索引，其中一个用于检索，一个用于排序 – 只能用到其中一个索引，5.6以上有ICP特性优化器判断使用哪个索引 表关联字段类型要一样（也包括长度一样），否则会有类型隐式转换 数据库优化内存优化注意内存的使用优化：系统与软件-&gt;内存使用考量 查看/分析资源消耗SHOW PROFILE 和 SHOW PROFILES 说明 作用域是会话级； SHOW PROFILES显示最近发给服务器的多条语句，条数根据会话变量profiling_history_size定义，默认是15，最大值为100。设为0等价于关闭分析功能。 默认是关闭；其是否启用是根据会话级的变量profiling mysql5.7以后不建议使用SHOW PROFILE指令，或直接从INFORMATION_SCHEMA.PROFILING中查看，建议利用PERFORMANCE_SCHEMA中的几个视图查看。（因为会被废弃） 开启1mysql&gt; SET profiling=1;或 SET profiling=on; 关闭1mysql&gt; SET profiling=0;或 SET profiling=off; 用法1SHOW PROFILE [type [, type] … ][FOR QUERY n][LIMIT row_count [OFFSET offset]] type是可选的，取值范围可以如下： ALL 显示所有性能信息 BLOCK IO 显示块IO操作的次数 CONTEXT SWITCHES 显示上下文切换次数，不管是主动还是被动 CPU 显示用户CPU时间、系统CPU时间 IPC 显示发送和接收的消息数量 MEMORY [暂未实现] PAGE FAULTS 显示页错误数量 SOURCE 显示源码中的函数名称与位置 SWAPS 显示SWAP的次数 SHOW PROFILE FOR QUERY n，这里的n就是对应SHOW PROFILES输出中的Query_ID。如果没有指定FOR QUERY，那么输出最近一条语句的信息。 SHOW PROFILE ALL FOR QUERY 2 的信息还可以通过SELECT * FROM information_schema.profiling WHERE query_id = 2 ORDER BY seq;获取。 参考：http://dev.mysql.com/doc/refman/5.5/en/show-profile.html 查看执行状态查看命令1SHOW [FULL] PROCESSLIST SHOW PROCESSLIST显示哪些线程正在运行。您也可以使用mysqladmin processlist语句得到此信息。如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程（也就是，与您正在使用的MySQL账户相关的线程）。如果您不使用FULL关键词，则只显示每个查询的前100个字符。 Command官方文档：https://dev.mysql.com/doc/refman/5.6/en/thread-commands.html Sleep通常代表资源未释放，如果是通过连接池，sleep状态应该恒定在一定数量范围内 实例:因前端数据输出时（特别是输出到用户终端）未及时关闭数据库连接，导致因网络连接速度产生大量sleep连接，在网速出现异常时，数据库 too many connections挂死。 简单解读，数据查询和执行通常只需要不到0.01秒，而网络输出通常需要1秒左右甚至更长，原本数据连接在0.01秒即可释放，但是因为前端程序未执行close操作，直接输出结果，那么在 结果未展现在用户桌面前，该数据库连接一直维持在sleep状态！ Waiting for net / reading from net / writing to net偶尔出现无妨，如大量出现，迅速检查数据库到前端的网络连接状态和流量Reading from net 表示server端正通过网络读取客户端发送过来的请求建议：减小客户端发送数据包大小，提高网络带宽/质量Writing to net通过网络传输数据 案例: 因外挂程序，内网数据库大量读取，内网使用的百兆交换迅速爆满，导致大量连接阻塞在waiting for net，数据库连接过多崩溃 Locked有更新操作锁定，通常使用innodb可以很好的减少locked状态的产生，但是切记，更新操作要正确使用索引，即便是低频次更新操作也不能疏忽。如上影响结果集范例所示。在myisam的时代，locked是很多高并发应用的噩梦。所以mysql官方也开始倾向于推荐innodb。 Copy to tmp table索引及现有结构无法涵盖查询条件，才会建立一个临时表来满足查询要求，产生巨大的恐怖的i/o压力。很可怕的搜索语句会导致这样的情况，如果是数据分析，或者半夜的周期数据清理任务，偶尔出现，可以允许。频繁出现务必优化之。执行ALTER TABLE修改表结构时建议：放在凌晨执行或者采用类似pt-osc工具通常与连表查询有关，建议逐渐习惯不使用连表查询。 实战范例：某社区数据库阻塞，求救，经查，其服务器存在多个数据库应用和网站，其中一个不常用的小网站数据库产生了一个恐怖的copy to tmp table 操作，导致整个硬盘i/o和cpu压力超载。Kill掉该操作一切恢复。 Copying to tmp table拷贝数据到内存中的临时表，常见于GROUP BY操作时,建议：创建适当的索引 Copying to tmp table on disk临时结果集太大，内存中放不下，需要将内存中的临时表拷贝到磁盘上，形成 *sql.MYD、*sql.MYI（在5.6及更高的版本，临时表可以改成InnoDB引擎了，可以参考选项default_tmp_storage_engine）建议：创建适当的索引，并且适当加大sort_buffer_size/tmp_table_size/max_heap_table_size Creating sort index当前的SELECT中需要用到临时表在进行ORDER BY排序，建议：创建适当的索引 Sending data并不是发送数据，别被这个名字所欺骗，这是从物理磁盘获取数据的进程，如果你的影响结果集较多，那么就需要从不同的磁盘碎片去抽取数据，偶尔出现该状态连接无碍。回到上面影响结果集的问题，一般而言，如果sending data连接过多，通常是某查询的影响结果集过大，也就是查询的索引项不够优化。前文提到影响结果集对SQL查询效率线性相关，主要就是针对这个状态的系统开销。如果出现大量相似的SQL语句出现在show proesslist列表中，并且都处于sending data状态，优化查询索引，记住用影响结果集的思路去思考。从server端发送数据到客户端，也有可能是接收存储引擎层返回的数据，再发送给客户端，数据量很大时尤其经常能看见备注：Sending Data不是网络发送，是从硬盘读取，发送到网络是Writing to net 建议：通过索引或加上LIMIT，减少需要扫描并且发送给客户端的数据量 Creating tmp table创建基于内存或磁盘的临时表，当从内存转成磁盘的临时表时，状态会变成：Copying to tmp table on disk建议：创建适当的索引，或者少用UNION、视图(VIEW)、子查询(SUBQUERY)之类的，确实需要用到临时表的时候，可以在session级临时适当调大 tmp_table_size/max_heap_table_size 的值 Storing result to query cache出现这种状态，如果频繁出现，使用set profiling分析，如果存在资源开销在SQL整体开销的比例过大（即便是非常小的开销，看比例），则说明query cache碎片较多使用flush query cache 可即时清理，也可以做成定时任务Query cache参数可适当酌情设置。 Freeing items理论上这玩意不会出现很多。偶尔出现无碍如果大量出现，内存，硬盘可能已经出现问题。比如硬盘满或损坏。i/o压力过大时，也可能出现Free items执行时间较长的情况。 Sorting for …和Sending data类似，结果集过大，排序条件没有索引化，需要在内存里排序，甚至需要创建临时结构排序。 Sorting result正在对结果进行排序，类似Creating sort index，不过是正常表，而不是在内存表中进行排序建议：创建适当的索引 statistics进行数据统计以便解析执行计划，如果状态比较经常出现，有可能是磁盘IO性能很差建议：查看当前io性能状态，例如iowait Waiting for global read lockFLUSH TABLES WITH READ LOCK整等待全局读锁建议：不要对线上业务数据库加上全局读锁，通常是备份引起，可以放在业务低谷期间执行或者放在slave服务器上执行备份 Waiting for tables,Waiting for table flushFLUSH TABLES, ALTER TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE, OPTIMIZE TABLE等需要刷新表结构并重新打开建议：不要对线上业务数据库执行这些操作，可以放在业务低谷期间执行 Waiting for lock_type lock等待各种类型的锁： Waiting for event metadata lock Waiting for global read lock Waiting for schema metadata lock Waiting for stored function metadata lock Waiting for stored procedure metadata lock Waiting for table level lock Waiting for table metadata lock Waiting for trigger metadata lock 建议：比较常见的是上面提到的global read lock以及table metadata lock，建议不要对线上业务数据库执行这些操作，可以放在业务低谷期间执行。如果是table level lock，通常是因为还在使用MyISAM引擎表，赶紧转投InnoDB引擎吧，别再老顽固了 State官方文档:http://dev.mysql.com/doc/refman/5.6/en/general-thread-states.html After create当线程创建表（包括临时表） Analyzing线程正在分析MYISAM的key分布 checking permissions线程正在检查执行语句的权限 Checking table线程正在执行表check操作 Cleaning up线程已经处理完一个命令然后开始准备释放内存和reset状态变量 Closing tables线程把改变的表数据flush到磁盘然后关闭使用的表。这个操作应该是很快的，如果这个状态长时间出现，要留意检查磁盘的状态。 Converting HEAP to MyISAM把一个内存中的临时表转换到磁盘上的MYISAM表 Copy to tmp table线程执行alter table语句。这个状态出现在新的表结构已经创建但数据还在拷贝到新表之前。 Copying to group table如果一个语句的group by和order by 条件不同，数据通过group by来排序然后拷贝到临时表 Copying to tmp table拷贝到内存中的临时表 Copying to tmp table on disk如果临时表过大，服务器要把内存中的临时表拷贝到磁盘。 Creating index对一个MYISAM 执行ALTER TABLE … ENABLE KEYS Creating sort index线程通过执行一个临时表来执行select语句 Creating table线程正在创建表（包括临时表） Creating tmp table创建临时表（在内存或者磁盘）上，如果表一开始在内存中到后面太大，临时表就会转换到磁盘上，而且状态也会变成Copying to tmp table on disk deleting from main table服务器正在执行一个多表delete语句的第一部分，也就是从第一张表删除数据，并且保留行以及偏移量数据以用来删除其他表中的数据。 deleting from reference tables服务器正在执行一个多表delete语句的二部分 discard_or_import_tablespace线程正在执行ALTER TABLE … DISCARD TABLESPACE 或者ALTER TABLE … IMPORT TABLESPACE 语句. end这个状态出现在ALTER TABLE, CREATE VIEW, DELETE, INSERT, SELECT, UPDATE语句结束之后，但在清理以前。 executing线程开始执行一个语句 Execution of init_command线程在init_command系统变量中执行语句 freeing items线程已经执行命令。一些释放 Flushing tables这个线程执行了FLUSH TABLES，而且等待所有的线程关闭表 FULLTEXT initialization服务器准备执行文本搜索 Killed发送了kill 语句给这个查询。这个语句在下次检查kill标志的时候，这个语句就应该放弃掉 Locked查询被其他的查询锁住了 logging slow query线程正在把慢sql写到慢查询log文件中 login直到线程授权成功以前都是这个状态 manage keys服务器正在管理表的索引 Opening tables, Opening table线程正在打开表。这个操作是很快的，除非有其他原因阻止了打开操作，例如ALTER TABLE LOCK TABLE optimizing服务器正在进行一个查询的初始阶段优化 preparing正在进行查询优化 Purging old relay logs线程删除不需要的relay log文件 query end这个状态出现在处理完以后但在冻结item之前 Reading from net服务器从网络读取包 Removing duplicates查询使用了select distinct。mysq在发送数据到客户端之前需要一个额外的过程来删除重复的行 removing tmp table在执行了select语句以后，线程正在删除内部的临时表 rename线程正在重命名表 rename result table线程正在执行一个alter table语句，而且已经创建了新表，对新表重命名来替换原始表。 Reopen tables线程获取了一个表的锁。由于它已经得知它依赖的表结构已经发生了变化。线程需要释放锁、关闭表、然后尝试重新打开他。 Repair by sorting修复代码通过排序来创建索引。 Repair done线程完成了对一个myisam表的多线程修复。 Repair with keycache修复代码正在通过key的缓存创建key。 Rolling back线程正在回滚事物。 Saving statemyisam的analysis和repair操作中，线程会把表的一些信息例如表的行数、AUTO_INCREMENT的计数器以及key的分布都保存到.MYI文件的头部 Searching rows for update语句执行的第一个阶段，找到所有满足条件的行记录 Sending data线程在读取和处理SELECT语句，发送数据到客户端。由于语句需要大量的磁盘访问，这个状态会在语句的整个生命周期中占据最长的一个状态。 setup线程开始进行ALTER TABLE 语句 Sorting for group线程正在为group by 执行排序 Sorting for order线程正在为order by 执行排序 Sorting index在myisam表的优化操作中，进行索引页的排序以便获得更好的访问性能。 Sorting result对结果进行排序 Statistics服务器正在计算统计数据从而来生成一个执行计划。如果一个线程保留这个状态很长的时间，意味着服务器在执行其他的磁盘相关的工作。 System lock线程正在请求和获取一个内部和外部锁。如果有这个状态 Table lockSystem Lock后的另一个线程状态。线程已经获取了一个外部锁，然后接下来要去获取一个内部表锁。 update线程已经准备好去更新 Updating现在正在找或者正在更新行 updating main table服务器正在执行一个多表update语句，正在更新第一张表，保存行和偏移量以用来更新其他表。 updating reference tables服务器正在执行一个多表update语句的第二部分，正在从其他表更新行 User lock请求或者等待获取一个锁 User sleep线程sleep Waiting for release of readlock等待一个全局的读锁 Waiting for tables, Waiting for table线程获得提示依赖的表结构发生了改变，线程需要重新打开表来获取新的结构。但是，重新打开表是需要等待其他的线程关闭表。 Waiting on cond线程正在等条件变成true Waiting to get readlock线程用FLUSH TABLES WITH READ LOCK语句，所以要获得一个全局的读锁，这个状态表示正在等这个锁。 Writing to net服务器把包写到网络 查看mysql状态show status 与 show global status 区别status：当前连接的状态（当前会话/连接）global status：mysql 启动后到目前的状态（全局会话） show status 详解官方文档：http://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html 状态名 作用域 详细解释 Aborted_clients Global 由于客户端没有正确关闭连接导致客户端终止而中断的连接数 Aborted_connects Global 试图连接到MySQL服务器而失败的连接数 Binlog_cache_disk_use Global 使用临时二进制日志缓存但超过binlog_cache_size值并使用临时文件来保存事务中的语句的事务数量 Binlog_cache_use Global 使用临时二进制日志缓存的事务数量 Bytes_received Both 从所有客户端接收到的字节数。 Bytes_sent Both 发送给所有客户端的字节数。 com* 各种数据库操作的数量 Compression Session 客户端与服务器之间只否启用压缩协议 Connections Global 试图连接到(不管是否成功)MySQL服务器的连接数 Created_tmp_disk_tables Both 服务器执行语句时在硬盘上自动创建的临时表的数量 Created_tmp_files Global mysqld已经创建的临时文件的数量 Created_tmp_tables Both 服务器执行语句时自动创建的内存中的临时表的数量。如果Created_tmp_disk_tables较大，你可能要增加tmp_table_size值使临时表基于内存而不基于硬盘 Delayed_errors Global 用INSERT DELAYED写的出现错误的行数(可能为duplicate key)。 Delayed_insert_threads Global 使用的INSERT DELAYED处理器线程数。 Delayed_writes Global 写入的INSERT DELAYED行数 Flush_commands Global 执行的FLUSH语句数。 Handler_commit Both 内部提交语句数 Handler_delete Both 行从表中删除的次数。 Handler_discover Both MySQL服务器可以问NDB CLUSTER存储引擎是否知道某一名字的表。这被称作发现。Handler_discover说明通过该方法发现的次数。 Handler_prepare Both A counter for the prepare phase of two-phase commit operations. Handler_read_first Both 索引中第一条被读的次数。如果较高，它建议服务器正执行大量全索引扫描；例如，SELECT col1 FROM foo，假定col1有索引。 Handler_read_key Both 根据键读一行的请求数。如果较高，说明查询和表的索引正确。 Handler_read_next Both 按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。 Handler_read_prev Both 按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY … DESC。 Handler_read_rnd Both 根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。 Handler_read_rnd_next Both 在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。 Handler_rollback Both 内部ROLLBACK语句的数量。 Handler_savepoint Both 在一个存储引擎放置一个保存点的请求数量。 Handler_savepoint_rollback Both 在一个存储引擎的要求回滚到一个保存点数目。 Handler_update Both 在表内更新一行的请求数。 Handler_write Both 在表内插入一行的请求数。 Innodb_buffer_pool_pages_data Global 包含数据的页数(脏或干净)。 Innodb_buffer_pool_pages_dirty Global 当前的脏页数。 Innodb_buffer_pool_pages_flushed Global 要求清空的缓冲池页数 Innodb_buffer_pool_pages_free Global 空页数。 Innodb_buffer_pool_pages_latched Global 在InnoDB缓冲池中锁定的页数。这是当前正读或写或由于其它原因不能清空或删除的页数。 Innodb_buffer_pool_pages_misc Global 忙的页数，因为它们已经被分配优先用作管理，例如行锁定或适用的哈希索引。该值还可以计算为Innodb_buffer_pool_pages_total - Innodb_buffer_pool_pages_free - Innodb_buffer_pool_pages_data。 Innodb_buffer_pool_pages_total Global 缓冲池总大小（页数）。 Innodb_buffer_pool_read_ahead_rnd Global InnoDB初始化的“随机”read-aheads数。当查询以随机顺序扫描表的一大部分时发生。 Innodb_buffer_pool_read_ahead_seq Global InnoDB初始化的顺序read-aheads数。当InnoDB执行顺序全表扫描时发生。 Innodb_buffer_pool_read_requests Global InnoDB已经完成的逻辑读请求数。 Innodb_buffer_pool_reads Global 不能满足InnoDB必须单页读取的缓冲池中的逻辑读数量。 Innodb_buffer_pool_wait_free Global 一般情况，通过后台向InnoDB缓冲池写。但是，如果需要读或创建页，并且没有干净的页可用，则它还需要先等待页面清空。该计数器对等待实例进行记数。如果已经适当设置缓冲池大小，该值应小。 Innodb_buffer_pool_write_requests Global 向InnoDB缓冲池的写数量。 Innodb_data_fsyncs Global fsync()操作数。 Innodb_data_pending_fsyncs Global 当前挂起的fsync()操作数。 Innodb_data_pending_reads Global 当前挂起的读数。 Innodb_data_pending_writes Global 当前挂起的写数。 Innodb_data_read Global 至此已经读取的数据数量（字节）。 Innodb_data_reads Global 数据读总数量。 Innodb_data_writes Global 数据写总数量。 Innodb_data_written Global 至此已经写入的数据量（字节）。 Innodb_dblwr_pages_written Global 已经执行的双写操作数量 Innodb_dblwr_writes Global 双写操作已经写好的页数 Innodb_log_waits Global 我们必须等待的时间，因为日志缓冲区太小，我们在继续前必须先等待对它清空 Innodb_log_write_requests Global 日志写请求数。 Innodb_log_writes Global 向日志文件的物理写数量。 Innodb_os_log_fsyncs Global 向日志文件完成的fsync()写数量。 Innodb_os_log_pending_fsyncs Global 挂起的日志文件fsync()操作数量。 Innodb_os_log_pending_writes Global 挂起的日志文件写操作 Innodb_os_log_written Global 写入日志文件的字节数。 Innodb_page_size Global 编译的InnoDB页大小(默认16KB)。许多值用页来记数；页的大小很容易转换为字节。 Innodb_pages_created Global 创建的页数。 Innodb_pages_read Global 读取的页数。 Innodb_pages_written Global 写入的页数。 Innodb_row_lock_current_waits Global 当前等待的待锁定的行数。 Innodb_row_lock_time Global 行锁定花费的总时间，单位毫秒。 Innodb_row_lock_time_avg Global 行锁定的平均时间，单位毫秒。 Innodb_row_lock_time_max Global 行锁定的最长时间，单位毫秒。 Innodb_row_lock_waits Global 一行锁定必须等待的时间数。 Innodb_rows_deleted Global 从InnoDB表删除的行数。 Innodb_rows_inserted Global 插入到InnoDB表的行数。 Innodb_rows_read Global 从InnoDB表读取的行数。 Innodb_rows_updated Global InnoDB表内更新的行数。 Key_blocks_not_flushed Global 键缓存内已经更改但还没有清空到硬盘上的键的数据块数量。 Key_blocks_unused Global 键缓存内未使用的块数量。你可以使用该值来确定使用了多少键缓存 Key_blocks_used Global 键缓存内使用的块数量。该值为高水平线标记，说明已经同时最多使用了多少块。 Key_read_requests Global 从缓存读键的数据块的请求数。 Key_reads Global 从硬盘读取键的数据块的次数。如果Key_reads较大，则Key_buffer_size值可能太小。可以用Key_reads/Key_read_requests计算缓存损失率。 Key_write_requests Global 将键的数据块写入缓存的请求数。 Key_writes Global 向硬盘写入将键的数据块的物理写操作的次数。 Last_query_cost Session 用查询优化器计算的最后编译的查询的总成本。用于对比同一查询的不同查询方案的成本。默认值0表示还没有编译查询。默认值是0。Last_query_cost具有会话范围。 Max_used_connections Global 服务器启动后已经同时使用的连接的最大数量。 ndb* ndb集群相关 Not_flushed_delayed_rows Global 等待写入INSERT DELAY队列的行数。 Open_files Global 打开的文件的数目。 Open_streams Global 打开的流的数量(主要用于记录)。 Open_table_definitions Global 缓存的.frm文件数量 Open_tables Both 当前打开的表的数量。 Opened_files Global 文件打开的数量。不包括诸如套接字或管道其他类型的文件。 也不包括存储引擎用来做自己的内部功能的文件。 Opened_table_definitions Both 已经缓存的.frm文件数量 Opened_tables Both 已经打开的表的数量。如果Opened_tables较大，table_cache 值可能太小。 Prepared_stmt_count Global 当前的预处理语句的数量。(最大数为系统变量: max_prepared_stmt_count) Qcache_free_blocks Global 查询缓存内自由内存块的数量。 Qcache_free_memory Global 用于查询缓存的自由内存的数量。 Qcache_hits Global 查询缓存被访问的次数。 Qcache_inserts Global 加入到缓存的查询数量。 Qcache_lowmem_prunes Global 由于内存较少从缓存删除的查询数量。 Qcache_not_cached Global 非缓存查询数(不可缓存，或由于query_cache_type设定值未缓存)。 Qcache_queries_in_cache Global 登记到缓存内的查询的数量。 Qcache_total_blocks Global 查询缓存内的总块数。 Queries Both 服务器执行的请求个数，包含存储过程中的请求。 Questions Both 已经发送给服务器的查询的个数。 Rpl_status Global 失败安全复制状态(还未使用)。 Select_full_join Both 没有使用索引的联接的数量。如果该值不为0,你应仔细检查表的索引 Select_full_range_join Both 在引用的表中使用范围搜索的联接的数量。 Select_range Both 在第一个表中使用范围的联接的数量。一般情况不是关键问题，即使该值相当大。 Select_range_check Both 在每一行数据后对键值进行检查的不带键值的联接的数量。如果不为0，你应仔细检查表的索引。 Select_scan Both 对第一个表进行完全扫描的联接的数量。 Slave_heartbeat_period Global 复制的心跳间隔 Slave_open_temp_tables Global 从服务器打开的临时表数量 Slave_received_heartbeats Global 从服务器心跳数 Slave_retried_transactions Global 本次启动以来从服务器复制线程重试次数 Slave_running Global 如果该服务器是连接到主服务器的从服务器，则该值为ON。 Slow_launch_threads Both 创建时间超过slow_launch_time秒的线程数。 Slow_queries Both 查询时间超过long_query_time秒的查询的个数。 Sort_merge_passes Both 排序算法已经执行的合并的数量。如果这个变量值较大，应考虑增加sort_buffer_size系统变量的值。 Sort_range Both 在范围内执行的排序的数量。 Sort_rows Both 已经排序的行数。 Sort_scan Both 通过扫描表完成的排序的数量。 ssl＊ ssl连接相关 Table_locks_immediate Global 立即获得的表的锁的次数。 Table_locks_waited Global 不能立即获得的表的锁的次数。如果该值较高，并且有性能问题，你应首先优化查询，然后拆分表或使用复制。 Threads_cached Global 线程缓存内的线程的数量。 Threads_connected Global 当前打开的连接的数量。 Threads_created Global 创建用来处理连接的线程数。如果Threads_created较大，你可能要增加thread_cache_size值。缓存访问率的计算方法Threads_created/Connections。 Threads_running Global 激活的（非睡眠状态）线程数。 Uptime Global 服务器已经运行的时间（以秒为单位）。 Uptime_since_flush_status Global 最近一次使用FLUSH STATUS的时间（以秒为单位）。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"分析问题","date":"2017-11-17T02:15:00.000Z","path":"数据库/Mysql/分析问题/","text":"基本认识 要学会怎样分析问题，而不是单纯拍脑袋优化 慢查询只是最基础的东西，要学会优化0.01秒的查询请求。 当发生连接阻塞时，不同状态的阻塞有不同的原因，要找到原因，如果不对症下药，就会南辕北辙 范例：如果本身系统内存已经超载，已经使用到了swap，而还在考虑加大缓存来优化查询，那就是自寻死路了。 影响结果集是非常重要的中间数据和优化指标，学会理解这一概念，理论上影响结果集与查询效率呈现非常紧密的线性相关。 监测与跟踪要经常做，而不是出问题才做 读取频繁度抽样监测 全监测不要搞，i/o吓死人。 按照一个抽样比例抽样即可。 针对抽样中发现的问题，可以按照特定SQL在特定时间内监测一段全查询记录，但仍要考虑i/o影响。 写入频繁度监测 基于binlog解开即可，可定时或不定时分析。 微慢查询抽样监测 高并发情况下，查询请求时间超过0.01秒甚至0.005秒的，建议酌情抽样记录。 连接数预警监测 连接数超过特定阈值的情况下，虽然数据库没有崩溃，建议记录相关连接状态。 学会通过数据和监控发现问题，分析问题，而后解决问题顺理成章。特别是要学会在日常监控中发现隐患，而不是问题爆发了才去处理和解决。 常见关注的重点慢查询日志 是否锁定，及锁定时间如存在锁定，则该慢查询通常是因锁定因素导致，本身无需优化，需解决锁定问题。 影响结果集如影响结果集较大，显然是索引项命中存在问题，需要认真对待。 Explain 操作 索引项使用不建议用using index做强制索引，如未如预期使用索引，建议重新斟酌表结构和索引设置。 影响结果集这里显示的数字不一定准确，结合之前提到对数据索引的理解来看，还记得嘛？就把索引当作有序序列来理解，反思SQL。 Set profiling , show profiles for query操作 执行开销 注意，有问题的SQL如果重复执行，可能在缓存里，这时要注意避免缓存影响。通过这里可以看到。 执行时间超过0.005秒的频繁操作SQL建议都分析一下。 深入理解数据库执行的过程和开销的分布 Show processlist 执行状态监控这是在数据库负载波动时经常进行的一项操作具体参见：查看执行状态 排查步骤确认问题详细了解问题状况 Too many connections 是常见表象，有很多种原因。 索引损坏的情况在innodb情况下很少出现。 如出现其他情况应追溯日志和错误信息。 确认瓶颈常见的瓶颈 cpu 非常高–索引问题 or 并发太高（通常是索引问题）并发问题：thread_pool 限制连接数（其他分支都支持线程池） 内存 swap高 – 内存分配不足（过多会 oom）适量调整内存，不要太大也不要太小，50%-70%比较保守，80%太偏激，尽量覆盖全部热点数据mysql 专用服务器，关闭 swap iowait 太高 – 内存不足 or io 设备性能太低 or 索引不当 or 频繁读取 select or 频繁排序/分组 了解基本运营状况 当前每秒读请求 当前每秒写请求 当前在线用户 当前数据容量 了解基本负载情况学会使用这些指令 Top –系统状态，哪个服务/进程消耗的 cpu 、内存 Vmstat – 查看 cpu、io、内存负载 dstat – 和 vmstat 类似，结果更友好 sar – systat 工具包的一个命令；关注（sar -u / -d / -r）cpu/io/内存 iotop – 查看哪个进程 io 消耗最高 oprofile – 神器，一般用不上，用法自己查 strace – 跟踪进程执行时的系统调用和所接收的信号 uptime iostat df Cpu负载构成 特别关注i/o压力( wa%) 多核负载分配 内存占用 Swap分区是否被侵占，若Swap分区被侵占，物理内存是否较多空闲 磁盘状态 硬盘满和inode节点满的情况要迅速定位和迅速处理 查看 mysql 的状态当前连接数 Netstat –an|grep 3306|wc –l Show processlist 当前连接分布 show processlist 前端应用请求数据库不要使用root帐号！ Root帐号比其他普通帐号多一个连接数许可。 前端使用普通帐号，在too many connections的时候root帐号仍可以登录数据库查询 show processlist! 记住，前端应用程序不要设置一个不叫root的root帐号来糊弄！非root账户是骨子里的，而不是名义上的。 状态分布 不同状态代表不同的问题，有不同的优化目标。 雷同SQL的分布 是否较多雷同SQL出现在同一状态 当前是否有较多慢查询日志 是否锁定 影响结果集 常用命令 slow log优先频次高，其次耗时久 show global status 查看连接数（活跃、不活跃），设置 interactive_timeout、wait_timeout 的 timecount值，减少不活跃连接 TPS、QPS、DML_Active tps=(handler_commit_d+handler_rollback_d)/uptime_d qps=(questions_d2 - questions_d1)/uptime_d dml_active=(com_select_d+com_insert_d+com_update_d+com_delete_d)/uptime_d 各种 buffer、cache 的利用率、命中率 innodb_buffer_pool_wait_free&gt;0 说明 innodb_buffer 不够用,有 wait、wait_free 发生，都要关注下 innodb_row_lock_current_waits 当前行锁的个数 slow_queries 慢查询的次数 table_locks_immediate 表锁的次数 table_locks_waited 表锁等待的次数 show processlist show engine innodb status主要看锁，事务，等待 pt-ioprofile可以查看哪个表在频繁的读写 频繁度分析 写频繁度 如果i/o压力高，优先分析写入频繁度 Mysqlbinlog 输出最新binlog文件，编写脚本拆分 最多写入的数据表是哪个 最多写入的数据SQL是什么 是否存在基于同一主键的数据内容高频重复写入？ 涉及架构优化部分，参见架构优化-缓存异步更新 读取频繁度 如果cpu资源较高，而i/o压力不高，优先分析读取频繁度 程序中在封装的db类增加抽样日志即可，抽样比例酌情考虑，以不显著影响系统负载压力为底线。 最多读取的数据表是哪个 最多读取的数据SQL是什么 该SQL进行explain 和set profiling判定 注意判定时需要避免query cache影响 比如，在这个SQL末尾增加一个条件子句 and 1=1 就可以避免从query cache中获取数据，而得到真实的执行状态分析。 是否存在同一个查询短期内频繁出现的情况 涉及前端缓存优化 制定方案抓大放小，解决显著问题 不苛求解决所有优化问题，但是应以保证线上服务稳定可靠为目标。 解决与评估要同时进行，新的策略或解决方案务必经过评估后上线。 测试方案确认优化方案的覆盖范围，不要为了解决1%的问题而忽略了99%的问题，并且不能带来新的问题（索引滥用，太多索引，导致 dml 效率降低）而且最好再测试环境验证通过后再上线 实施方案回顾反馈案例分析服务器出现too many connections 阻塞入手点： 查看服务器状态，cpu占用，内存占用，硬盘占用，硬盘i/o压力 查看网络流量状态，mysql与应用服务器的输入输出状况 通过Show processlist查看当前运行清单 注意事项，日常应用程序连接数据库不要使用root账户，保证故障时可以通过root 进入数据库查看 show processlist。 状态分析： 参见如上执行状态清单，根据连接状态的分布去确定原因。 紧急恢复 在确定故障原因后，应通过kill掉阻塞进程的方式 立即恢复数据库。 善后处理：以下针对常见问题简单解读 Sleep 连接过多导致，应用端及时释放连接，排查关联因素。 Locked连接过多，如源于myisam表级锁，更innodb引擎;如源于更新操作使用了不恰当的索引或未使用索引，改写更新操作SQL或建立恰当索引。 Sending data连接过多，用影响结果集的思路优化SQL查询，优化表索引结构。 Free items连接过多，i/o压力过大 或硬盘故障 Waiting for net , writing to net 连接过多， mysql与应用服务器连接阻塞。 其他仍参见如上执行状态清单所示分析。 如涉及不十分严格安全要求的数据内容，可用定期脚本跟踪请求进程，并kill掉僵死进程。如数据安全要求较严格，则不能如此进行。 数据库负载过高，响应缓慢入手点： 查看cpu状态，服务器负载构成 可能问题1：i/o占用过高 步骤1： 检查内存是否占用swap分区，排除因内存不足导致的i/o开销。 步骤2：通过iostat 指令分析i/o是否集中于数据库硬盘，是否是写入度较高。 步骤3：如果压力来自于写，使用mysqlbinlog 解开最新的binlog文件。 步骤4：编写日志分析脚本或grep指令，分析每秒写入频度和写入内容。 写入频度不高，则说明i/o压力另有原因或数据库配置不合理。 步骤5：编写日志分析脚本或grep 指令，分析写入的数据表构成，和写入的目标构成。 步骤6：编写日志分析脚本，分析是否存在同一主键的重复写入。 比如出现大量 update post set views=views+1 where tagid=的操作，假设在一段时间内出现了2万次，而其中不同的tagid有1万次，那么就是有50%的请求是重复update请求，有可以通过异步更新合并的空间。 提示一下，以上所提及的日志分析脚本编写，正常情况下不应超过1个小时，而对系统负载分析所提供的数据支持价值是巨大的，对性能优化方案的选择是非常有意义的，如果您认为这项工作是繁冗而且复杂的工作，那么一定是在分析思路和目标把握上出现了偏差。 可能问题2：i/o占用不高，CPU 占用过高 步骤1：查看慢查询日志 步骤2：不断刷新查看Show processlist清单，并把握可能频繁出现的处于Sending data状态的SQL。 步骤3：记录前端执行SQL 于前端应用程序执行查询的封装对象内，设置随机采样，记录前端执行的SQL，保证有一定的样本规模，并且不会带来前端i/o负载的激增。 基于采样率和记录频率，获得每秒读请求次数数据指标。 编写日志分析脚本，分析采样的SQL构成，所操作的数据表，所操作的主键。 对频繁重复读取的SQL(完全一致的SQL)进行判定，是否数据存在频繁变动，是否需要实时展现最新数据，如有可能，缓存化，并预估缓存命中率。 对频繁读取但不重复的(SQL结构一致，但条件中的数据不一致)SQL进行判定，是否索引足够优化，影响结果集与输出结果是否足够接近。 步骤4：将导致慢查询的SQL或频繁出现于show processlist状态的SQL，或采样记录的频繁度SQL进行分析，按照影响结果集的思路和索引理解来优化。 步骤5：对如上难以界定问题的SQL进行 set profiling 分析。 步骤6：优化后分析继续采样跟踪分析。并跟踪比对结果。 善后处理 日常跟踪脚本，不断记录一些状态信息。保证每个时间节点都能回溯。 确保随时能了解服务器的请求频次，读写请求的分布。 记录一些未造成致命影响的隐患点，可暂不解决，但需要记录。 如确系服务器请求频次过高，可基于负载分布决定硬件扩容方案，比如i/o压力过高可考虑固态硬盘；内存占用swap可考虑增加内容容量等。用尽可能少的投入实现最好的负载支撑能力，而不是简单的买更多服务器。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":" 架构与表设计","date":"2017-11-16T15:35:00.000Z","path":"数据库/Mysql/架构与表设计/","text":"数据表设计存储引擎 默认使用innodb 引擎，基本抛弃 myisam（只读也不一定比 innodb 快） text/blob 垂直拆分后，转成 myisam 表，innodb 存 blob 会产生大量的磁盘碎片 各存储引擎区别：存储引擎 字符集默认使用utf-8 注意字符集问题，server=&gt;database(trigger、stored procedure、event scheduler)=&gt;table=&gt;column不要同时指定字符集（character set）和校验集（collect set），避免出现和默认对应关系不一致 字段类型基本原则 越小越好 字段都NOT NULL 默认都加上NOT NULL约束，必须为NULL用0表示 在对该字段进行COUNT(*)统计时，统计结果更准确（值为NULL的不会被COUNT统计进去） 存储技巧 可使用枚举类型ENUM的内部存储机制是采用TINYINT或SMALLINT（并非CHAR/VARCHAR），性能一点都不差 存储ipv4地址用INT UNSIGNED存储IPV4地址，用INET_ATON()、INET_NTOA()进行转换 存储日期时间一个常识性误导：建议用TIMESTAMP取代DATETIME。从5.6开始，建议优先选择DATETIME存储日期时间。因为它的可用范围比TIMESTAMP更大，物理存储上仅比TIMESTAMP多1个字节，整体性能上的损失并不大。 InnoDB表行记录物理长度不超过8KBInnoDB的data page默认是16KB的情况下。当实际存储长度超过8KB（尤其有TEXT/BLOB列）且读写频繁的话, 则最好把这些列拆分到子表中，不要和主表放在一起存储。如果不太频繁，可以考虑继续保留在主表中[1]。 索引 显式指定自增 int/bigint unsigned not null 作为主键(有主键 TPS 提升9%，不要使用 uuid，性能差) 选择适当的索引顺序，选择性高条件靠前 基数（ Cardinality ）很低（30%以下）的字段不创建索引（MySQL还不支持 bitmap 索引） 常用排序（ORDER BY）、分组（GROUP BY）、取唯一（DISTINCT）字段上创建索引 单表索引数量不超过5个 不使用外键 超过20个长度的字符串列（不需要排序），创建前缀索引而非整列索引[6] 优点:有效提高索引利用率 缺点:对这个列排序时用不到前缀索引 高级特性使用 是否使用分区表？在可以提升性能或者运维便利性的场景下，还是建议使用分区表。例如：日志系统，按时间纬度进行分区，方便删除历史数据 是否使用存储过程、触发器？单机可用，分布式舍弃。（存储器场景：不变的业务逻辑） 是否使用视图？MySQL因为没有物化视图，因此视图尽量少用（不用） 缓存与数据库结合读取优缺点优点：可以减少数据库读取请求缺点：增加代码复杂度、增加维护难度 如何使用？ 评估进入缓存的数据规模，以及命中率优化[2] 不是所有数据都适合被缓存，也并不是进入了缓存就意味着效率提升 命中率是第一要评估的数据；核心在于如何判断哪些属于热点数据 善于利用内存，请注意数据存储的格式及压缩算法。 如何确定热点数据见下文 写入优缺点优点：减少数据库写入i/o压力缺点：数据不能第一时间持久化，有丢失风险 如何使用?缓存实时更新，数据库异步更新（使用队列,请注意使用increment来维持队列序号） 不要通过get-&gt;处理数据-&gt;set-&gt;更新数据的方式维护队列[3]。 使用increment存储队列编号，用标记+编号作为key存储队列内容[4]。 后台基于缓存队列读取更新数据并更新数据库[5] 基于队列读取后可以合并更新 更新合并率是重要指标 异步更新风险 前后端同时写，可能导致覆盖风险。 使用后端异步更新，前端应用程序不要写数据库，否则可能造成写入冲突。 一种兼容的解决方案是，前端和后端不要写相同的字段[7]。 缓存数据丢失或服务崩溃可能导致数据丢失风险。 如缓存中间出现故障，则缓存队列数据不会回写到数据库，而用户会认为已经完成，此时会带来比较明显的用户体验问题。 一个不彻底的解决方案是，确保高安全性，高重要性数据实时数据更新，而低安全性数据通过缓存异步回写方式完成。此外，使用相对数值操作而不是绝对数值操作更安全[8][9]。 异步更新如出现队列阻塞可能导致数据丢失风险。 异步更新通常是使用缓存队列后，在后台由cron或其他守护进程写入数据库。 如果队列生成的速度&gt;后台更新写入数据库的速度，就会产生阻塞，导致数据越累计越多，数据库响应迟缓，而缓存队列无法迅速执行，导致溢出或者过期失效。 解决办法：使用 MQ 队列产品而不使用 memcache 来进行缓存异步更新 反范式设计（冗余） 适度冗余可以减少查询请求 适度冗余可以解决分表带来的索引查询问题 适度冗余可以解决统计类负载较高的查询问题 适度冗余可以减少 io 请求频次，提高 io 支撑能力(cpu 换 io) 反范式设计的概念 无外键，无连表查询，强调索引,去关联化 不考虑触发器及其他内部的存储过程 便于分布式设计，允许适度冗余，为了容量扩展允许适度开销。 基于业务自由优化，基于i/o 或查询设计，无须遵循范式结构设计。 冗余结构设计所面临的典型场景 原有展现程序涉及多个表的查询，希望精简查询程序 数据表拆分往往基于主键，而原有数据表往往存在非基于主键的关键查询，无法在分表结构中完成。 存在较多数据统计需求（count, sum等），效率低下。 冗余造成的问题一致性问题–业务层校验 冗余设计方案基于展现的冗余设计冗余特征：字段简单，更改度不高为了简化展现程序，在一些数据表中往往存在冗余字段[13] 基于查询的冗余设计 涉及分表操作后，一些常见的索引查询可能需要跨表，带来不必要的麻烦。确认查询请求远大于写入请求时，应设置便于查询项的冗余表。 冗余表要点 数据一致性，简单说：同增，同删，同更新。 可以做全冗余，或者只做主键关联的冗余，比如通过用户名查询uid，再基于uid查询源表。 实战示例[10][11][12] 基于统计的冗余结构 为了减少会涉及大规模影响结果集的表数据操作，比如count，sum操作。应将一些统计类数据通过冗余数据结构保存。 冗余数据结构可能以字段方式存在，也可能以独立数据表结构存在，但是都应能通过源数据表恢复。 实战示例[13] 基于 io 压力优化的冗余现象： 单次请求多次写入的情况 请求频次较高，io 压力较大 存在高频读取请求，数据可靠性要求高 可用方案: 数据压缩存储 写入缓存队列 通过冗余结构，合并为一次写入[14][15] 历史数据表历史数据表对应于热点数据表，将需求较少又不能丢弃的数据存入，仅在少数情况下被访问。 主从架构基本认识 读写分离对负载的减轻远远不如分库分表来的直接。 写压力会传递给从表，只读从库一样有写压力，一样会产生读写锁！ 一主多从结构下，主库是单点隐患，很难解决（如主库当机，从库可以响应读写，但是无法自动担当主库的分发功能） 主从延迟也是重大问题。一旦有较大写入问题，如表结构更新，主从会产生巨大延迟。 优点部署简单 局限性 io 压力无法分布，性价比不高（主是多线程写，从是单线程写；分担读压力，不能分担写压力） 同步延时避免不了 一主多从，主库单点，很难自动故障转移（一从转主，其他从不能自动关联到这个新主） 应用场景一主一从 在线热备 异地分布 写分布，读统一。 仍然困难重重，受限于网络环境问题巨多！ 自动障碍转移 主崩溃，从自动接管 个人建议，负载均衡主要使用分库方案，主从主要用于热备和障碍转移。 潜在优化点为了减少写压力，有些人建议主不建索引提升i/o性能，从建立索引满足查询要求。个人认为这样维护较为麻烦。而且从本身会继承主的i/o压力，因此优化价值有限。该思路特此分享，不做推荐(caoz观点)。 分布式设计 设计分布式之前，先优化单机： 单机读 qps 几千很容易 单机写 qps 几千很容易 数据量最少在3000万以上（具体业务具体分析） 目标防止单点隐患lvs nginx（轮询，自动切换） 所谓单点隐患，就是某台设备出现故障，会导致整体系统的不可用，这个设备就是单点隐患。 理解连带效应，所谓连带效应，就是一种问题会引发另一种故障，举例而言，memcache+mysql是一种常见缓存组合，在前端压力很大时，如果memcache崩溃，理论上数据会通过mysql读取，不存在系统不可用情况，但是mysql无法对抗如此大的压力冲击，会因此连带崩溃。因A系统问题导致B系统崩溃的连带问题，在运维过程中会频繁出现。[16][17] 连带效应是常见的系统崩溃，日常分析崩溃原因的时候需要认真考虑连带效应的影响，头疼医头，脚疼医脚是不行的。 方便系统扩容 数据容量增加后，要考虑能够将数据分布到不同的服务器上。 请求压力增加时，要考虑将请求压力分布到不同服务器上。 扩容设计时需要考虑防止单点隐患。 安全可控，成本可控 数据安全，业务安全 人力资源成本 &gt; 带宽流量成本 &gt; 硬件成本 成本与流量的关系曲线应低于线性增长（流量为横轴，成本为纵轴）。 规模优势 分库&amp;分表优点 负载分担较好 不存在同步延迟 拆分方法灵活 局限性 需要有备份和自动故障转移的方案 需要应用端配合，无法完全满足关联查询的需求 推荐方案 以分库、分表为负载和数据支撑方案 以主从结构为热备和故障转移方案 使用中间件作为分布式数据库的前端（amoeba，要去关联化） 数据一致性问题解决 前端校验 后端 cron 定时跑数据 基本认识 用分库&amp;拆表是解决数据库容量问题的唯一途径。 分库&amp;拆表也是解决性能压力的最优选择。 分库 – 不同的数据表放到不同的数据库服务器中（也可能是虚拟服务器） 拆表 – 一张数据表拆成多张数据表，可能位于同一台服务器，也可能位于多台服务器（含虚拟服务器）。 去关联化原则 摘除数据表之间的关联，是分库的基础工作。 摘除关联的目的是，当数据表分布到不同服务器时，查询请求容易分发和处理。 学会理解反范式数据结构设计，所谓反范式，第一要点是不用外键，不允许Join操作，不允许任何需要跨越两个表的查询请求。第二要点是适度冗余减少查询请求[18] 去关联化处理会带来额外的考虑，比如说，某一个数据表内容的修改，对另一个数据表的影响。这一点需要在程序或其他途径去考虑。 分库方案安全性拆分运维优化，易于管理 将高安全性数据与低安全性数据分库，这样的好处第一是便于维护，第二是高安全性数据的数据库参数配置可以以安全优先，而低安全性数据的参数配置以性能优先。参见运维优化相关部分。 安全性开启参数：sync_binlog，innodb_flush_log_at_trx_commit = 2(丢失最后1s 数据);开启后性能下降10~100倍 基于业务逻辑拆分易于管理，对应用端友好，负载不能均分 根据数据表的内容构成，业务逻辑拆分，便于日常维护和前端调用。 基于业务逻辑拆分，可以减少前端应用请求发送到不同数据库服务器的频次，从而减少链接开销。 基于业务逻辑拆分，可保留部分数据关联，前端web工程师可在限度范围内执行关联查询。 基于负载压力拆分负载相对可以均摊；管理不方便 基于负载压力对数据结构拆分，便于直接将负载分担给不同的服务器。 基于负载压力拆分，可能拆分后的数据库包含不同业务类型的数据表，日常维护会有一定的烦恼。 混合拆分组合 基于安全与业务拆分为数据库实例，但是可以使用不同端口放在同一个服务器上。 基于负载可以拆分为更多数据库实例分布在不同数据库上 例如: 基于安全拆分出A数据库实例， 基于业务拆分出B,C数据库实例， C数据库存在较高负载，基于负载拆分为C1,C2,C3,C4等 实例。 数据库服务器完全可以做到 A+B+C1为一台，C2,C3,C4各单独一台。 分表方案数据量过大或者访问压力过大的数据表需要切分 纵向分表（常见为忙闲分表） 单数据表字段过多，可将频繁更新的整数数据与非频繁更新的字符串数据切分[19] 过于频繁的，使用 nosql（memcached，redis 等） 横向切表 等分切表，如哈希切表或其他基于对某数字取余的切表。等分切表的优点是负载均分；缺点是当容量继续增加时无法方便的扩容，需要重新进行数据的切分或转表。而且一些关键主键不易处理。（只能基于差存条件进行拆分，否则没法查询） 递增切表，比如每1kw用户开一个新表，优点是可以适应数据的自增趋势；缺点是往往新数据负载高（最新的表），压力分配不平均。 日期切表，适用于日志记录式数据，优缺点等同于递增切表。 个人倾向于递增切表，具体根据应用场景决定。 热点数据分表将数据量较大的数据表中将读写频繁的数据抽取出来，形成热点数据表。通常一个庞大数据表经常被读写的内容往往具有一定的集中性，如果这些集中数据单独处理，就会极大减少整体系统的负载。 热点数据表与旧有数据关系: 可以是一张冗余表，即该表数据丢失不会妨碍使用，因源数据仍存在于旧有结构中。优点是安全性高，维护方便，缺点是写压力不能分担，仍需要同步写回原系统。 可以是非冗余表，即热点数据的内容原有结构不再保存，优点是读写效率全部优化；缺点是当热点数据发生变化时，维护量较大。 具体方案选择需要根据读写比例决定，在读频率远高于写频率情况下，优先考虑冗余表方案。 数据存储：热点数据表可以用单独的优化的硬件存储，比如昂贵的闪存卡或大内存系统。 热点数据表的重要指标: 热点数据的定义需要根据业务模式自行制定策略，常见策略为，按照最新的操作时间；按照内容丰富度等等。 数据规模，比如从1000万条数据，抽取出100万条热点数据。 热点命中率，比如查询10次，多少次命中在热点数据内。 理论上，数据规模越小，热点命中率越高，说明效果越好。需要根据业务自行评估。 热点数据表的动态维护: 加载热点数据方案选择 定时从旧有数据结构中按照新的策略获取 在从旧有数据结构读取时动态加载到热点数据 剔除热点数据方案选择 基于特定策略，定时将热点数据中访问频次较少的数据剔除 如热点数据是冗余表，则直接删除即可，如不是冗余表，需要回写给旧有数据结构。 通常，热点数据往往是基于缓存或者key-value方案冗余存储，所以这里提到的热点数据表，其实更多是理解思路，用到的场合可能并不多。（适合组合条件场景，数据规模中等，像淘宝的数据量就需要第三方搜索引擎） 参考资料 [MySQL优化案例]系列 — 优化InnoDB表BLOB列的存储效率 《Mysql 性能优化教程-曹政》 1.[MySQL优化案例]系列 — 优化InnoDB表BLOB列的存储效率 ↩2.实景分析： 前端请求先连接缓存，缓存未命中连接数据库，进行查询，未命中状态比单纯连接数据库查询多了一次连接和查询的操作；如果缓存命中率很低，则这个额外的操作非但不能提高查询效率，反而为系统带来了额外的负载和复杂性，得不偿失。 ↩3.范例:$var=Memcache_get($memcon,”var”);$var++;memcache_set($memcon,”var”,$var);这样一个脚本，使用apache ab去跑，100个并发，跑10000次，然后输出缓存存取的数据，很遗憾，并不是1000，而是5000多，6000多这样的数字，中间的数字全在 get &amp; set的过程中丢掉了。原因：读写间隔中其他并发写入，导致数据丢失。 ↩4.范例2:用memcache_increment来做这个操作，同样跑测试会得到完整的10000，一条数据不会丢。 ↩5.实战范例：某论坛热门贴，前端不断有views=views+1数据更新请求。缓存实时更新该状态后台任务对数据库做异步更新时，假设执行周期是5分钟，那么五分钟可能会接收到这样的请求多达数十次乃至数百次，合并更新后只执行一次update即可。类似操作还包括游戏打怪，生命和经验的变化；个人主页访问次数的变化等。 ↩6.例如：ALTER TABLE t1 ADD INDEX(user(20))前缀索引的长度可以基于对该字段的统计得出，一般略大于平均长度一点就可以了。 ↩7.实战范例：用户在线上时，后台异步更新用户状态。管理员后台屏蔽用户是直接更新数据库。结果管理员屏蔽某用户操作完成后，因该用户在线有操作，后台异步更新程序再次基于缓存更新用户状态，用户状态被复活，屏蔽失效。 ↩8.范例：支付信息，道具的购买与获得，一旦丢失会对用户造成极大的伤害。而经验值，访问数字，如果只丢失了很少时间的内容，用户还是可以容忍的。 ↩9.范例：如果使用 Views=Views+…的操作，一旦出现数据格式错误，从binlog中反推是可以进行数据还原，但是如果使用Views=特定值的操作，一旦缓存中数据有错误，则直接被赋予了一个错误数据，无法回溯！ ↩10.实战范例1: 用户分表将用户库分成若干数据表;基于用户名的查询和基于uid的查询都是高并发请求。用户分表基于uid分成数据表，同时基于用户名做对应冗余表。如果允许多方式登陆，可以有如下设计方法：uid,passwd,用户信息等等，主数据表，基于uid分表ukey,ukeytype,uid 基于ukey分表，便于用户登陆的查询。分解成如下两个SQL:select uid from ulist_key_13 where ukey=’$username’ and ukeytype=‘login’;select from ulist_uid_23 where uid=$uid and passwd=’$passwd’;ukeytype定义用户的登陆依据，比如用户名，手机号，邮件地址，网站昵称等。 Ukey+ukeytype 必须唯一此种方式需要登陆密码统一，对于第三方connect接入模式，可以通过引申额外字段完成 ↩11.实战范例2：用户游戏积分排名表结构 uid,gameid,score 参见前文实时积分排行。表内容巨大，需要拆表。需求1：基于游戏id查询积分排行需求2：基于用户id查询游戏积分记录解决方案：建立完全相同的两套表结构，其一以uid为拆表主键，其二以gameid为拆表主键，用户提交积分时，向两个数据结构同时提交。 ↩12.实战范例3：全冗余查询结构主信息表仅包括 主键及备注memo字段（text类型），只支持主键查询，可以基于主键拆表。所以需要展现和存储的内容均在memo字段重体现。对每一个查询条件，建立查询冗余表，以查询条件字段为主键，以主信息表主键id 为内容。日常查询只基于查询冗余表，然后通过in的方式从主信息表获得内容。优点是结构扩展非常方便，只需要扩展新的查询信息表即可，核心思路是，只有查询才需要独立的索引结构，展现无需独立字段。缺点是只适合于相对固定的查询架构，对于更加灵活的组合查询束手无策。 ↩13.举例，信息表 message，存在字段 fromuid,touid,msg,sendtime 四个字段，其中 touid+sendtime是复合索引。存在查询为 select from message where touid=$uid order by sendtime desc limit 0,30;展示程序需要显示发送者姓名，此时通常会在message表中增加字段fromusername，甚至有的会增加fromusersex，从而无需连表查询直接输出信息的发送者姓名和性别。这就是一种简单的，为了避免连表查询而使用的冗余字段设计。 ↩13.实战范例：论坛板块的发帖量，回帖量，每日新增数据等。网站每日新增用户数等。参见Discuz论坛系统数据结构，有较多相关结构。参见前文分段积分结构，是典型用于统计的冗余结构。后台可以通过源数据表更新该数字。Redis的Zset类型可以理解为存在一种冗余统计结构。 ↩14.游戏组队，5个武将。（建立临时的武将表，定时更新到主表） ↩15.实时统计 ↩16.实战范例： 在mysql连接不及时释放的应用环境里，当网络环境异常（同机房友邻服务器遭受拒绝服务攻击，出口阻塞），网络延迟加剧，空连接数急剧增加，导致数据库连接过多崩溃。 ↩17.实战范例：前端代码 通常我们封装 mysql_connect和memcache_connect，二者的顺序不同，会产生不同的连带效应。如果mysql_connect在前，那么一旦memcache连接阻塞，会连带mysql空连接过多崩溃。 ↩18.比如说，信息表，fromuid, touid, message字段外，还需要一个fromuname字段记录用户名，这样查询者通过touid查询后，能够立即得到发信人的用户名，而无需进行另一个数据表的查询。 ↩19.范例: user表 ，个人简介，地址，QQ号，联系方式，头像 这些字段为字符串类型，更新请求少； 最后登录时间，在线时常，访问次数，信件数这些字段为整数型字段，更新频繁，可以将后面这些更新频繁的字段独立拆出一张数据表，表内容变少，索引结构变少，读写请求变快。 ↩","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"基础知识","date":"2017-11-16T15:24:00.000Z","path":"数据库/Mysql/基础知识/","text":"索引Mysql 常用数据结构 B+ Tree Hash index 二者区别b+treeB+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的节点间有指针相互链接。 在B+树上的常规检索，从根节点到叶子节点的搜索效率基本相当，不会出现大幅波动，而且基于索引的顺序扫描时，也可以利用双向指针快速左右移动，效率非常高。 B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大 Hash index简单地说，哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 哈希索引只能使用 in ，=，!= 使用场景 如果是等值查询，那么哈希索引明显有绝对优势 如果是范围查询检索，这时候哈希索引就毫无用武之地了 哈希索引也没办法利用索引完成排序 哈希索引也不支持多列联合索引的最左匹配规则 在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题 FAQ为什么使用索引能提高效率关系型数据库的数据索引（Btree及常见索引结构）的存储是有序的（在有序的情况下，通过索引查询一个数据是无需遍历索引记录的）。 「默认 b+tree索引」查询效率趋近于二分法，趋近于 log2(N)。 「hash 索引」查询效率是寻址操作，趋近于1次查询，比有序索引查询效率更高 如何理解数据索引的结构从结构 数据索引通常默认采用b+tree索引，（内存表使用了hash索引）。 单向 有序 排序序列是查找效率最高的（二分查找），使用树形索引的目的是为了达到快速的更新和增删操作。 在极端情况下（比如数据查询需求量非常大，而数据更新需求极少，实时性要求不高，数据规模有限），直接使用 单一排序序列，折半查找速度最快。 从优化 在进行索引分析和SQL优化时，可以将数据索引字段想象为单一有序序列，并以此作为分析的基础。 涉及到复合索引情况，复合索引按照索引顺序拼凑成一个字段，想象为单一有序序列，并以此作为分析的基础。 从查询优化器 一条数据查询只能使用一个索引，索引可以是多个字段合并的复合索引。但是一条数据查询不能使用多个索引。 拓展阅读 MySQL索引背后的数据结构及算法原理 MySQL索引与Index Condition Pushdown FAQ系列 | MySQL索引之聚集索引 FAQ系列 | MySQL索引之主键索引 Mysql加密mysql用户名密码公式(5.7以前)：1concat('*',sha1(unhex(sha1('mypassword')))) //等于 password('mypassword') 记录密码方式 binlog 记录密码方式 5.6以前 binlog 会打印明文密码 5.6以后 以密文方式记录 binlog 的命令 1234567CREATE USER … IDENTIFIED BY …GRANT … IDENTIFIED BY …SET PASSWORD …SLAVE START … PASSWORD = … (as of 5.6.4)CREATE SERVER … OPTIONS(… PASSWORD …) (as of 5.6.9)ALTER SERVER … OPTIONS(… PASSWORD …) (as of 5.6.9)//change master to 不在此范畴中，在启动主从时会有 warning 提示 影响结果集1explain [sql] 影响因素查询条件与索引的关系决定影响结果集。 影响结果集是什么？影响结果集不是输出结果数，不是查询返回的记录数，而是索引所扫描的结果数。 优化目标 影响结果集越趋近于实际输出或操作的目标结果集，索引效率越高。 影响结果集与查询开销的关系可以理解为线性相关。减少一半影响结果集，即可提升一倍查询效率！当一条搜索query可以符合多个索引时，选择影响结果集最少的索引。 如何优化？SQL的优化，核心就是对结果集的优化，认识索引是增强对结果集的判断，基于索引的认识，可以在编写SQL的时候，对该SQL可能的影响结果集有预判，并做出适当的优化和调整。 Limit 的影响 如果索引与查询条件和排序条件完全命中，影响结果集就是limit后面的数字（$start + $end），比如 limit 200,30 影响结果集是230. 而不是30. 如果索引只命中部分查询条件，甚至无命中条件，在无排序条件情况下，会在索引命中的结果集 中遍历到满足所有其他条件为止。 比如 select * from user limit 10; 虽然没用到索引，但是因为不涉及二次筛选和排序，系统直接返回前10条结果，影响结果集依然只有10条，就不存在效率影响。 如果搜索所包含的排序条件没有被索引命中，则系统会遍历所有索引命中的结果，并且排序。 例如 Select from user order by timeline desc limit 10; 如果timeline不是索引，影响结果集是全表，就存在需要全表数据排序，这个效率影响就巨大。再比如 Select from user where area=’厦门’ order by timeline desc limit 10; 如果area是索引，而area+timeline未建立索引，则影响结果集是所有命中 area=’厦门’的用户，然后在影响结果集内排序。 Cpu/内存/磁盘利用特点Cpu利用特点每个连接对应一个线程，每个并发query只能使用到一个核 &lt;5.1，多核心支持较弱 5.1，可利用4个核 5.5，可利用24个核 5.6，可利用64个核 内存利用特点 类似ORACLE的SGA(全局)、PGA（会话）模式，注意PGA不宜分配过大 SGA：System Global Area是Oracle Instance的基本组成部分，在实例启动时分配;系统全局域SGA主要由三部分构成：共享池、数据缓冲区、日志缓冲区。PGA：Process Global Area是为每个连接到Oracle database的用户进程保留的内存Mysql 中会话内存分配不要太大，否则会造成 OOM（out of memory） 内存管理简单、有效。在高TPS、高并发环境下，可增加物理内存以减少物理IO，提高并发性能 Mysql 锁并发竞争比较严重，MariaDB、Percona进行优化 2014年建议使用 Percona &gt; MariaDB &gt; Mysql 有类似ORACLE library cache的query cache，但效果不佳，建议关闭 Qcache有个全局锁,有数据变更，则会更新 Qcache；Dml 不频繁时（只读），Qcache 还是有用的–没有只读的场景，从服务也要写 执行计划没有缓存，每次都需要解析 SQL 通常内存建议按热点数据总量的15%-20%来规划，专用单实例则可以分配物理内存的50~70%左右 磁盘利用特点 binlog、redo log、undo log主要顺序IO datafile是主要是 「随机IO」 和顺序IO也有 OLTP业务（数据查询）以随机IO为主，建议加大内存，尽量合并随机IO为顺序IO 通过 innodb buffer 进行合并 OLAP业务（数据分析）以顺序IO为主，加大内存的同时增加硬盘数量提高顺序IO性能 MyISAM是堆组织表（HOT），InnoDB是索引组织表（IOT） MYISAM ：新写的数据放入堆的最后Innodb ：B+tree 的索引结构 InnoDB相比MyISAM更消耗磁盘空间 Innodb 的占用空间通常比 MyISAM 多1.5~2倍innodb 保存更多的数据，比如：事务信息 数据引擎Myisam速度快，响应快。表级锁是致命问题 表级锁 – 读和写是串行的 不支持事务 内存只缓存索引 Innodb目前主流存储引擎 行级锁 – 不同数据读写可并行处理 支持事务 内存缓存数据和索引 Crash Recovery – 故障自动修复，修复相比MyISAM速度更快 Memory使用 hash 索引 频繁更新和海量读取情况下仍会存在锁定状况 不支持范围查询及排序，只能=,!=,in 设计规范基本原则 减少物理I/O，让MySQL闲下来 (减少负载（前端+各级cache）) 转变随机I/O为顺序I/O （本地写队列，最后合并写） 减小活跃数据 (冷热数据分离) 分库分表 (垂直、水平拆分、分布式集群) 读写分离 (从只读，加大 buffer使读的性能更好些) OLTP、OLAP分离 禁止 禁止 Mysql 做运算 快速更新大数据表，禁止直接运行count(*)统计 数据量控制 单表行记录数控制在1000万以内，行平均长度控制在16KB以内，单表20GB以内 单实例下数据表数量不超过2000个， 单库下数据表数量不超过500个 读写 读写分离，主库只写和少量实时读取请求，统计在从库上执行 采用队列方式合并多次写请求， 持续写入， 避免瞬间压力 存储 「垂直拆分」超长text/blob进行垂直拆分，并先行压缩 「水平拆分」冷热数据进行水平拆分，LRU原则 压力分散，在线表和归档表（日志表）分开存储 SQL规范 注意引号问题会导致类型转换（where id = ‘1234’） 过滤用户提交SQL，防止注入 杜绝 like ‘%xxx%’，不用/少用 like ‘xxx%’ 不用/少用子查询，改造成连接（JOIN） 不用/少用FOR UPDATE、LOCK IN SHARE MODE，防止锁范围扩大化 参考资料 FAQ系列 | B+树索引和哈希索引的区别","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"},{"name":"Mysql","slug":"数据库/Mysql","permalink":"http://yoursite.com/categories/数据库/Mysql/"}]},{"title":"Git 常用命令","date":"2017-11-16T13:21:00.000Z","path":"GIT/git/","text":"工作流程图 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 GIT 配置Git的设置文件为.git/config，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 显示当前的Git配置:git config --list 编辑Git配置文件:git config -e [--global] 设置提交代码时的用户信息git config [--global] user.name [name]git config [--global] user.email [email address] 新建代码库 当前目录空目录/拷贝的 git 目录git init 创建目录git init [project-name] 下载一个项目(包含历史)git clone [url] 本地初始化远端git remote add origin [http://xxx.git]git push -u origin master 增加/删除文件 添加指定文件到暂存区git add [file1] [file2] ... 添加指定目录到暂存区，包括子目录git add [dir] 添加文件的部分到暂存区git add -e [file] 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ... 停止追踪指定文件，但该文件会保留在工作区git rm --cached [file] 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 代码提交 提交暂存区到仓库区git commit -m [message] 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message] 提交工作区自上次commit之后的变化，直接到仓库区git commit -a 提交时显示所有diff信息git commit -v 使用一次新的commit，替代上一次提交如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message] 重做上一次commit，并包括指定文件的新变化git commit --amend ... 修改提交信息git rebase HEAD^^ -i//-i 是交互模式//reword 修改注释信息//squash 与之前的注释合并 暂存数据（Stage） 把文件写入 Stagegit stash save [file] 查看 Stage 列表git stash list 应用最后一个 Stagegit stash pop 应用指定的 Stagegit stash apply [stash@{1}] 远程同步 下载远程仓库的所有变动git fetch [remote] 下载远程仓库的所有taggit fetch —tags 只下载远程仓库的分支git fetch [remote] &lt;本地master&gt;:&lt;新建分支名&gt; 显示所有远程仓库git remote -v 显示某个远程仓库的信息git remote show [remote] 增加一个新的远程仓库，并命名git remote add [shortname] [url] 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch] 上传本地指定分支到远程仓库git push [remote] [branch] 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force 推送所有分支到远程仓库git push [remote] --all 在已有项目中添加子项目git submodule add [remote] [dir] 撤销 恢复暂存区的指定文件到工作区git checkout [file] 恢复某个commit的指定文件到工作区git checkout [commit] [file] 恢复上一个commit的所有文件到工作区git checkout . 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file] 重置暂存区与工作区，与上一次commit保持一致git reset --hard 重置暂存区，保留已修改git reset —soft 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit] 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit] 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit] 新建一个commit，用来撤销指定commit#_后者的所有变化都将被前者抵消，并且应用到当前分支_git revert [commit] 分支 列出所有本地分支git branch 列出所有远程分支git branch -r 列出所有本地分支和远程分支git branch -a 新建一个分支，但依然停留在当前分支git branch [branch-name] 新建一个分支，并切换到该分支git checkout -b [branch] 新建一个分支，指向指定commitgit branch [branch] [commit] 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch] 切换到指定分支，并更新工作区git checkout [branch-name] 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch] 合并指定分支到当前分支git merge [branch] 保留 commit 信息（即使在 fast-forward模式）git merge –no-ff [branch] 合并多个commit为一次提交git merge —squash [branch] 选择一个commit，合并进当前分支git cherry-pick [commit] 删除分支git branch -d [branch-name] 删除远程分支git push origin --deletegit branch -dr 衍合分支到主干git checkout [branch]git rebase [master] 合并分支到主干git checkout [master]git merge [branch] 标签若 tag 和 branch 重名，操作时可指定，比如：tags/v2 列出所有taggit tag 新建一个tag在当前commitgit tag [tag] 新建一个tag在指定commitgit tag [tag] [commit] 查看tag信息git show [tag] 提交指定taggit push [remote] [tag] 提交所有taggit push [remote] --tags 新建一个分支，指向某个taggit checkout -b [branch] [tag] 查看信息多个条件默认是or 关系，使用 –all-match 变为 AND 关系 显示有变更的文件git status 显示当前分支的版本历史git log 显示commit历史，以及每次commit发生变更的文件git log --stat 显示某个文件的版本历史，包括文件改名git log --follow [file]git whatchanged [file] 显示指定文件相关的每一次diffgit log -p [file] 显示两个 tag 之间的版本历史git log [tag1]…[tag2] 显示指定时间范围的版本历史git log --since=2.months.ago --until=1.day.ago 显示在分支2而不在分支1上的版本历史（branch2没有提交的历史）git log [old_branch1]…[new_branch2] 搜索特定提交者git log —author=[name] 搜索历史信息git log —grep=&#39;search message&#39; 显示指定文件是什么人在什么时间修改过git blame [file] 显示暂存区和工作区的差异git diff 显示暂存区和上一个commit的差异git diff --cached 显示工作区与当前分支最新commit之间的差异git diff HEAD 显示两次提交之间的差异git diff [first-branch]...[second-branch] 显示某次提交的元数据和内容变化git show [commit] 显示某次提交发生变化的文件git show --name-only [commit] 显示某次提交时，某个文件的内容git show [commit]:[filename] 显示分支发生的变化git show [branch] 显示最后一次提交发生的变化git show HEAD 显示当前分支的最近几次提交git reflog 查找信息git grep [pattern] [search_scope] 查看分支间的关系git show-branch//(加号)表示所在分支 包含 此行所标识的 commit//(空格)表示所在分支 不包含 此行所标识的 commit//(减号)表示所在分支是经过 merge 得到的,而所在行的内容即是 merge 的基本信息//(星号)表示如果需要在某列标识+(加号),且此列为当前分支所在列,那么则将+(加号)转变为*(星号) 显示两个分支的差异git whatchanged -p [branch1]…[branch2] 其他命令 仓库移了位置git remote set-url origin &lt;new repo addr&gt; 生成一个可供发布的压缩包git archive 压缩信息，清理垃圾git gc 一致性检查git fsck 查看对象数据库#四种对象类型：blob、commit、tag、tree#commit 指向 tree#tree 显示一个目录的状态，指向 tree 或者 blob 显示对象类型git cat-file -t [ID] 显示对象信息git cat-file [type] [ID] 显示 tree 信息git ls-tree [ID] 忽略本地修改 忽略本地修改（仓库存在）git update-index --assume-unchanged [file] 删除忽略的本地修改git update-index --no-assume-unchanged [file] 意外情况 找回删除的文件git reflog / git fsck --lost-foundgit checkout [hash值] 从本地分支生成 patch（用于 email 提交）git format-patch origin 与主干同步git fetch origingit rebase origin/master 提交到了错误的分支git branch experimental //创建一个指向当前master的位置的指针git reset --hard master~3 //移动master分支的指针到3个版本之前git checkout experimental","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}],"categories":[{"name":"GIT","slug":"GIT","permalink":"http://yoursite.com/categories/GIT/"}]},{"title":"TCP/IP 协议","date":"2017-11-11T03:00:00.000Z","path":"计算机网络/tcp-ip-协议/","text":"指代整个协议族；之所以叫TCP/IP协议，是因为TCP、IP是核心的协议。 传输层TCP 协议定义 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义 数据传输 三次握手WHY 为什么要三次握手？为了防止已失效的客户端连接请求突然又传送到了服务端，若没有客户端确认，则会占用服务端的资源。 HOW 客户端 传输 服务端 SYN(x) -&gt; SYN(x) SYN(y)-ACK(x+1) &lt;- SYN(y)+ACK(x+1) ACK(y+1) -&gt; ACK(Y+1) 必须是客户端发起的SYN SYN: 同步序列编号（Synchronize Sequence Numbers）ACK: 确认编号（Acknowledgement Number） 四次挥手WHY 为什么要四次挥手？全双工通信，所以必须双方都确认 FIN 才可以关闭连接 HOW 客户端 传输 服务端 FIN(客户端传输完毕) -&gt; FIN ACK &lt;- ACK（先确认） FIN &lt;- FIN （服务端传输完毕） ACK -&gt; ACK 客户端、服务端都可以发起 FIN FIN: 数据传输完毕 (Finsh) 包头结构 名称 长度 源端口 16位 目标端口 16位 序列号 32位 回应序号 32位 TCP头长度 4位 reserved 6位 控制代码 6位 窗口大小 16位 偏移量 16位 校验和 16位 选项 32位(可选) TCP包头的最小长度，为20字节。 UDP 协议定义一种无连接的传输层协议；可靠性通过「应用层」保证。 数据传输不提供数据包分组、组装和不能对数据包进行排序 包头结构 名称 长度 源端口 16位 目的端口 16位 长度 16位 校验和 16位 UDP包头只有8个字节 常见的协议DNS 应用场景： 面向数据报方式 网络数据大多为短消息 拥有大量Client 对数据安全性无特殊要求 网络负担非常重，但对响应速度要求高 TCP vs UDP - TCP UDP 可靠性 可靠（面向连接） 不可靠（无连接） 有序性 有序 无序 占用资源 多 少 程序结构 复杂 简单 数据模式 数据流[1]（字节流） 数据报文[2] 通信模式 点对点 一对一，一对多，多对一，多对多 首部开销 20字节 8字节 通信信道 全双工 不可靠信道 网络层IP 协议作用IP协议是TCP/IP协议簇中的核心协议，也是TCP/IP的载体。所有的TCP，UDP，ICMP及IGMP数据都以 IP数据报 格式传输。IP提供不可靠的，无连接的数据传送服务；如果发生某种错误，IP会丢失该数据，然后发送ICMP消息给信源端 IP数据报 IP路由选择局域网经过ARP协议将目的主机的IP地址解析为MAC地址 非局域网主机通过IP数据报连接目的主机时，按照如下步骤搜索: 搜索路由表，优先搜索匹配主机，如果能找到和IP地址完全一致的目标主机，则将该包发向目标主机 搜索路由表，如果匹配主机失败，则匹配同子网的路由器，这需要子网掩码的协助。如果找到路由器，则将该包发向路由器。 搜索路由表，如果匹配同子网路由器失败，则匹配同网号路由器，如果找到路由器，则将该包发向路由器。 搜索路由表，如果以上都失败了，就搜索默认路由，如果默认路由存在，则发包 如果都失败了，就丢掉这个包。 如何查看路由表 参考资料 维基百科-TCP 百科-TCP CDSN-TCP IP协议-CSDN IP协议-百度经验 1.可以合并多个请求数据(放在缓冲区中)，一次/多次读取完 ↩2.发送几次请求，就需要接收几次；一次只能读取一个完整的报文 ↩","tags":[],"categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/categories/计算机网络/"}]},{"title":"网络参考模型","date":"2017-11-10T18:52:00.000Z","path":"计算机网络/网络参考模型/","text":"OSI 参考模型 国际标准化组织(ISO) 和 国际电报电话咨询委员会(CCITT) 联合制定了 OSI（Open System Interconnect） WHYOSI参考模型是计算机网路体系结构发展的产物；其目的是为异种计算机互连提供 一个共同的基础和标准框架 ，并为保持相关标准的一致性和兼容性提供共同的参考。 WHAT实现「开放系统互连」所建立的「通信功能」的「分层模型」；分七层：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 HOW 每一层的功能是独立的，利用下一层为上一层提供服务； 同等层实体之间通信由该层的协议管理。 除物理层之外，每层都会在原始数据前添加一串属于自己的协议头；接收端向上传递过程中会删除自己的协议头 服务：下一层向上一层提供的「通信功能」和「层之间的会话规定」。协议：两个开放系统中的 同等层之间的通信规则和约定 1～4层协议称为下层协议 5～7层协议称为上层协议 分层说明应用层 (Application)网络服务与最终用户的一个接口。协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP 表示层（Presentation Layer）数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）格式有，JPEG、ASCll、DECOIC、加密格式等 会话层（Session Layer）建立、管理、终止会话。（在五层模型里面已经合并到了应用层）对应主机进程，指本地主机与远程主机正在进行的会话 传输层 (Transport)定义传输数据的协议端口号，以及流控和差错校验。协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层 网络层 (Network)进行逻辑地址寻址，实现不同网络之间的路径选择。协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP 数据链路层 (Link)建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正。 物理层（Physical Layer）建立、维护、断开物理连接。（由底层网络定义协议） TCP/IP 参考模型 TCP/IP是先有了协议，才制定TCP/IP模型。目前是互联网的事实标准 WHYARPANET是由美国国防部DoD（U.S.Department of Defense）赞助的研究网络。逐渐地它通过租用的电话线连结了数百所大学和政府部门。当无线网络和卫星出现以后，现有的协议在和它们相连的时候出现了问题，所以需要一种新的参考体系结构。 WHAT由 TCP、IP 两个协议演进过来的，共分为四层：网络访问层、互联网层、传输层和应用层 HOW 分层说明应用层为用户提供所需要的各种服务协议有：HTTP、FTP、Telnet、DNS、SMTP等. 传输层为应用层提供端到端的通信功能协议有：TCP、UDP 网络互联层负责分配地址和传送二进制数据。协议有：IP、IGMP、ICMPIP协议是网际互联层最重要的协议，它提供的是一个可靠、无连接的数据报传递服务。 网络接口层负责监视数据在主机和网络之间的交换。事实上，TCP/IP本身并未定义该层的协议，所以其具体实现的方法随着网络类型的不同而不同。 OSI 比对 TCP/IP 参考资料 科来协议图 百科-七层模型 百科-TCP/IP参考模型","tags":[],"categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/categories/计算机网络/"}]},{"title":"欢迎来到贺龙飞的知识库！","date":"2017-11-08T13:20:17.000Z","path":"index/","text":"何为「知识库」？在这个信息爆炸的年代，获取信息的成本接近于0，每天都有很多新的信息蜂拥而来；过往针对这些信息，我都是放在云笔记中（尝试过有道、印象、为知、蚂蚁等），但是放在笔记中不等于掌握，所以需要进一步的进行归纳和整理，所以才有了「知识库」。 为什么要用 github page 作为「知识库」?之前都是放在云笔记中，但是不利于查找和分享，同时有些需求云笔记不太能满足我；这边整理了下自己对「知识库」的需求： 无限级分类 可以全文搜索 内容有目录 可以方便查询 便于进行知识梳理 保证数据不会丢失 内容有更新历史 所以最终选择了使用 github page。 「知识库」和「博客」有什么区别？个人学习的过程是：收集-&gt;整理-&gt;吸收-&gt;输出 收集：采用「evernote」进行信息的收集整理：采用「知识库」进行信息/知识的整理吸收：在工作/学习中「实战」输出：采用「博客」对知识进行深入的剖析/个人见解 综上所述，「知识库」是对知识的整理，「博客」是对知识的剖析","tags":[],"categories":[]},{"title":"YAR","date":"2017-11-07T14:35:00.000Z","path":"程序语言/PHP/yar/","text":"What一个轻量级可并行的 RPC 框架，支持三种打包协议（msgpack,json,php）,传输协议支持 http（tcp/unix以后会支持）​ github 地址：https://github.com/laruence/yarphp 官网：http://php.net/manual/zh/book.yar.php Why鸟哥举了两个场景(传统 web 应用)： 一个进程，一个请求，但是涉及到多个没有依赖性的数据源，只能串行处理（依次等待所有数据源处理）完毕后才能响应； yar 可以并行处理，减少时间开销 一个应用系统随着业务的增加，人员流失，只做加法，等到不可维护性的时候，只能重构 yar 可以给系统进行解耦（实际上是 soa 思想） Howserver放置在 web 服务器上，通过 http 访问(默认get访问会输出 doc 信息) client 串行调用 并行调用（yar 内部使用 libcurl + epoll ） 高级进阶Yar 协议分析Yar整个协议由82字节长度的yar_header + 8字节的数据打包协议(MSGPACK、JSON、PHP) + N字节的Body组成。 安全性 未验证 123456789typedef struct _yar_header &#123; unsigned int id; // transaction id unsigned short version; // protocl version unsigned int magic_num; // default is: 0x80DFEC60 unsigned int reserved; unsigned char provider[32]; // reqeust from who unsigned char token[32]; // request token, used for authentication unsigned int body_len; // request body len&#125; struct _yar_header中需要注意的是magic_num值。该值在Client与Server都应该保持一致。否则视为不合法的数据。我们可以通过修改这个值，定制一个yar框架，防止其他人恶意请求。 跨语言 利用 yar 可以接收 struct 实现 client 12345array( &quot;i&quot; =&gt; &apos;&apos;, // transaction id &quot;m&quot; =&gt; &apos;&apos;, // the method which being called &quot;p&quot; =&gt; array(), // parameters) server 1234567array( &quot;i&quot; =&gt; &apos;&apos;, &quot;s&quot; =&gt; &apos;&apos;, //status &quot;r&quot; =&gt; &apos;&apos;, //return value &quot;o&quot; =&gt; &apos;&apos;, //output &quot;e&quot; =&gt; &apos;&apos;, //error or exception) 参考资料 Yar – 并行的RPC框架(Concurrent RPC framework) YAR 并行RPC框架研究 Yar协议分析与跨语言实现","tags":[{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"},{"name":"yar","slug":"yar","permalink":"http://yoursite.com/tags/yar/"},{"name":"php_ext","slug":"php-ext","permalink":"http://yoursite.com/tags/php-ext/"}],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://yoursite.com/categories/程序语言/"},{"name":"PHP","slug":"程序语言/PHP","permalink":"http://yoursite.com/categories/程序语言/PHP/"}]},{"title":"MAC 必备软件","date":"2017-11-06T15:25:00.000Z","path":"效率工具/mac/","text":"系统 Vanilla：隐藏菜单栏 Spectacle：窗口分屏 Coffitivity：白噪音 Enpass：密码管理工具 TeamViewer：远程控制 Tickeys：打字音效 Lantern：出国工具 Irvue：自动切换壁纸 OmniDiskSweeper: 查看磁盘占用工具 学习 Anki：复习工具 Evernote：收集资料 Leanote：笔记 MarginNote Pro：学习 pdf 神器（收费） MindNode：思维导图（收费） kiwix：离线维基百科 工作 Atom：编辑器 Charles：抓包工具（收费） Dash：文档（收费） iTerm 2：终端 PhpStorm：PHP 的 IDE Sequel Pro：Mysql 客户端 SourceTree：Git 客户端 Sublime Text：轻量可定制编辑器 Typora：MD 编辑器","tags":[],"categories":[{"name":"效率工具","slug":"效率工具","permalink":"http://yoursite.com/categories/效率工具/"}]},{"title":"Blog 系统之 Hexo","date":"2017-11-05T06:26:00.000Z","path":"开源软件简析/blog/","text":"Blog 系统之前用的是 Octopress ，由于不更新了，且使用的是 ruby 开发，生成 html 比较慢，所以迁移到 hexo。 Hexo安装 hexohexo 文档 1234$ npm install -g hexo-cli$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 安装完毕后，请自行修改 /_config.yml 最后别忘记添加 CNAME 主题采用 Hiker 主题；[中文说明] [Github] 删除作者自己的 cnzz 统计代码 12345678910# source/js/scripts.js 中，删除以下代码var s = [ '&lt;div style=\"display: none;\"&gt;', '&lt;script src=\"https://s11.cnzz.com/z_stat.php?id=1260716016&amp;web_id=1260716016\" language=\"JavaScript\"&gt;&lt;/script&gt;', '&lt;/div&gt;' ].join(''); var di = $(s); $('#container').append(di); 修改主题的配置文件 12345# /themes/hiker/_config.yml# 清空以下信息，都是作者本人信息social:donate:# 其他数据请自行填充 Atom 配置 采用 atom 作为 md 的编辑器 详细配置见：atom-hexo配置 Hexo 命令 详细命令清单见：hexo 命令 新增文章1$ hexo new [layout] &lt;title&gt; 预览1$ hexo g 实时更新预览1$ hexo g -w 发布1$ hexo d -g 自定义命令配置 bash/zsh 的配置文件 .basrc/.zshrc 增加如下命令 123export BLOG_PATH=\"blog的目录\"alias blog_root=\"cd $BLOG_PATH/\"PATH=\"...:$BLOG_PATH/shell\" 进入 Hexo 根目录1blog_root 预览1blog_preview 生成 html1blog_generate 发布1blog_deploy 打开 atom1blog_atom","tags":[{"name":"atom","slug":"atom","permalink":"http://yoursite.com/tags/atom/"}],"categories":[{"name":"开源软件简析","slug":"开源软件简析","permalink":"http://yoursite.com/categories/开源软件简析/"}]},{"title":"Atom 编辑器","date":"2017-11-04T21:51:00.000Z","path":"开源软件简析/atom/","text":"安装插件 markdown-preview-enhanced ：增强版 md 预览 markdown-writer ：写博客必备利器 tool-bar ：工具栏基类 tool-bar-markdown-writer ： md-writer的工具栏 vim-mode-plus ：vim 插件 markdown-table-editor ：表格编辑工具 atom-terminal ：终端 tool-bar-markdown-writer 修改文件 ~/.atom/packages/tool-bar-markdown-writer/lib/tool-bar-markdown-writer.coffee 增加如下功能： 增加 publish-draft Preview markdown 增加支持 markdown-preview-enhanced 新增的配置如下：12345678910111213&#123; 'icon': 'content-duplicate' 'tooltip': 'Publish Draft' 'callback': 'markdown-writer:publish-draft'&#125;&#123; 'icon': 'markdown' 'tooltip': 'Preview Markdown' 'data': ['markdown-preview-enhanced', 'markdown-preview-plus','markdown-preview'] 'visible': (data) -&gt; pkg = data.find (pkg) -&gt; !!atom.packages.getLoadedPackage(pkg) \"#&#123;pkg&#125;:toggle\" if pkg&#125; Atom 配置文件123456789101112131415161718192021\"*\": core: packagesWithKeymapsDisabled: [ \"markdown-preview\" ] telemetryConsent: \"no\" \"exception-reporting\": userId: \"5a8289c3-5832-4d26-af7b-3235427b72da\" \"markdown-preview-enhanced\": enableExtendedTableSyntax: true imageDropAction: \"copy to image folder\" mathRenderingOption: \"MathJax\" \"markdown-writer\": fileExtension: \".md\" renameImageOnCopy: true \"tool-bar\": &#123;&#125; \"tool-bar-markdown-writer\": &#123;&#125; \"vim-mode-plus\": notifiedCoffeeScriptNoLongerSupportedToExtendVMP: true welcome: showOnStartup: false Simiki 配置 主要使用的是 md-writer 生成项目专用的配置文件： 1点击 packages -&gt; markdown-writer -&gt; configurations -&gt; create project configs 最终在项目的根目录下生成 _mdwriter.cson 修改如下配置： 1234567891011121314151617181920212223242526272829303132# Directory to drafts from siteLocalDirsiteDraftsDir: \"content/\"# Directory to posts from siteLocalDirsitePostsDir: \"content/\"# Directory to images from siteLocalDir# - E.g. to use the current filename directory, can use &#123;directory&#125;siteImagesDir: \"attach/img/&#123;year&#125;/&#123;month&#125;/\"# Filename format of new drafts creatednewDraftFileName: \"&#123;slug&#125;&#123;extension&#125;\"# Filename format of new posts creatednewPostFileName: \"&#123;slug&#125;&#123;extension&#125;\"# Front matter date format, determines the &#123;date&#125; in frontMatterfrontMatterDate: \"&#123;year&#125;-&#123;month&#125;-&#123;day&#125; &#123;hour&#125;:&#123;minute&#125;\"# Front matter templatefrontMatter: \"\"\"---title: \"&#123;title&#125;\"date: \"&#123;date&#125;\"layout: \"page\"---[TOC]\"\"\"# File extension of posts/draftsfileExtension: \".md\"# File slug separatorslugSeparator: \"-\"tableExtraPipes: true Hexo 配置 主要使用的是 md-writer 生成项目专用的配置文件： 1点击 packages -&gt; markdown-writer -&gt; configurations -&gt; create project configs 最终在项目的根目录下生成 _mdwriter.cson 修改如下配置： 12345678910111213141516171819202122232425262728293031323334353637siteEngine: \"hexo\"# Website URL of your blogsiteUrl: \"http://www.helongfei.com\"# Directory to drafts from siteLocalDirsiteDraftsDir: \"source/_drafts/\"# Directory to posts from siteLocalDirsitePostsDir: \"source/_posts/&#123;year&#125;/\"# Directory to images from siteLocalDir# - E.g. to use the current filename directory, can use &#123;directory&#125;siteImagesDir: \"source/images/&#123;year&#125;/&#123;month&#125;/\"# Filename format of new drafts creatednewDraftFileName: \"&#123;slug&#125;&#123;extension&#125;\"# Filename format of new posts creatednewPostFileName: \"&#123;slug&#125;&#123;extension&#125;\"# Front matter date format, determines the &#123;date&#125; in frontMatterfrontMatterDate: \"&#123;year&#125;-&#123;month&#125;-&#123;day&#125; &#123;hour&#125;:&#123;minute&#125;\"# Front matter templatefrontMatter: \"\"\"---layout: \"&#123;layout&#125;\"title: \"&#123;title&#125;\"date: \"&#123;date&#125;\"---\"\"\"# File extension of posts/draftsfileExtension: \".md\"# File slug separatorslugSeparator: \"-\"# Table row continuation# - Enable to auto insert table columns when you press enter in a table rowtableNewLineContinuation: true","tags":[{"name":"atom","slug":"atom","permalink":"http://yoursite.com/tags/atom/"}],"categories":[{"name":"开源软件简析","slug":"开源软件简析","permalink":"http://yoursite.com/categories/开源软件简析/"}]},{"title":"Wiki 系统之 Simiki","date":"2017-11-03T16:00:00.000Z","path":"开源软件简析/wiki/","text":"Wiki 系统需求：git+md 系统 实现方式 缺点 优点 dokuwiki php+文本 编辑器不好用；数据量大搜索可能有性能问题（txt） 插件丰富，txt移植方便 mediawiki php+mysql 配置复杂；visualeditor 加载慢 功能全面，visualeditor simiki git+md 全量生成html；图片支持不好 可定制 gitbook git+md 全量生成 html ( 渲染巨慢 ) 有编辑器 Simiki 安装simiki 文档 123pip install simikimkdir mywiki &amp;&amp; cd mywikisimiki init 配置 _config.yml 123456789101112url:title:keywords:description:author:theme: yasimple_x2markdown: - fenced_code - extra - codehilite(css_class=hlcode, linenums=False) - toc(title=Table of Contents) 增加 CNAME 文件 为了域名解析；文本内容为待解析域名 Atom 配置 使用 Atom 作为 md 编辑器 详细配置见 Atom-simike 配置 发布fab需要_config.yml增加如下配置项： 1234deploy: - type: git remote: origin branch: gh-pages 发布的时候如下命令： 1fab deploy #不支持python3 shell使用自定义命令： 1wiki_deploy simiki 命令生成 html1simiki g 预览 wiki1simiki p 实时生成 html1simiki p -w","tags":[{"name":"atom","slug":"atom","permalink":"http://yoursite.com/tags/atom/"}],"categories":[{"name":"开源软件简析","slug":"开源软件简析","permalink":"http://yoursite.com/categories/开源软件简析/"}]},{"title":"Codeception 之验收测试","date":"2016-05-02T07:38:09.000Z","path":"测试/codeception/","text":"Acceptance Testing验收测试针对整个站点进行测试，模拟真实用户的访问流程。 写测试的人员不需要知道网站的内部实现。 两种测试类型PhpBrowser配置tests/acceptance.suite.yml 123456class_name: AcceptanceTestermodules: enabled: - PhpBrowser: url: - \\Helper\\Acceptance Selenium WebDriver配置tests/acceptance.suite.yml 1234567class_name: AcceptanceTestermodules: enabled: - WebDriver: url: browser: chrome - \\Helper\\Acceptance 配置Selenium环境 下载Selenium Standalone Server 下载Google Chrome Driver 启动： 1234//启动selenium-serverjava -jar selenium-server-standalone-2.53.0.jar//启动Chrome Driver./chromedriver 编写测试 测试写在tests/acceptance,后缀为Cept，例如：SiginCept.php PhpBrowser测试文档参考：PHP Browser actions Assertions Grabbers Comments Cookies, Urls, Title Selenium WebDriver测试文档参考：Selenium WebDriver Session Snapshots 其他DB配置 SQL语句放在 /tests/_data 下 DB-config修改codeception.yml Debug 命令行增加 --debug 手动输出 codecept_debug() 手动下一步 pauseExecution 记录测试 Recorder extension 参考资料 Acceptance Testing介绍 PHP Browser介绍 PHP Browser文档 Selenium WebDriver文档","tags":[{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"},{"name":"Codeception","slug":"Codeception","permalink":"http://yoursite.com/tags/Codeception/"}],"categories":[{"name":"测试","slug":"测试","permalink":"http://yoursite.com/categories/测试/"}]},{"title":"RBAC","date":"2016-04-10T06:36:00.000Z","path":"权限控制/rbac/","text":"前言在权限设计中，若权限比较复杂，使用简单的权限判断，则会使代码比较冗长，逻辑比较复杂，不利用维护和扩展。这时可以考虑使用 RBAC。 RBAC 介绍RBAC（Role-Based Access Control ）基于角色的访问控制。抽象出来就是用户，角色和权利三个方面。在权利和用户之间，增加一个角色，使其不再耦合在一起，方便维护和扩展。 一个角色拥有多个权利，一个用户想拥有某些权利时，只需要附给用户相应的角色即可。若一些用户需要相同的角色，则建立起用户组即可。 RBAC 分类RBAC0 RBAC核心,其他分类都是建立在此基础上 主要有四部分组成： 用户 角色 权限 会话 用户和角色是多对多关系，角色和权限是多对多关系。 一个会话只能由一个用户创建，会话和角色是多对多关系。一般会有8张表来设计(若不需要用户组，则是5张表)： RBAC1 基于 RBAC0 的角色分级；角色有了继承和包含的层级 这种模型使用 RBAC0 也是可以实现的，但是会造成数据冗余 RBAC2 基于 RBAC0 的角色限制（互斥，数量限制等） RBAC3 组合 RBAC1 和 RBAC2 RBAC 总结通常使用 RBAC0 就可以解决大部分权限问题，其他的细节在具体的设计中具体分析。RBAC 只是一种设计思想，我们在设计的过程中要具体问题具体分析，灵活的使用，不要照本宣科。 参考资料 权限管理——RBAC模型总结 RBAC RBAC权限管理 基于角色的访问控制","tags":[{"name":"rbac","slug":"rbac","permalink":"http://yoursite.com/tags/rbac/"}],"categories":[{"name":"权限控制","slug":"权限控制","permalink":"http://yoursite.com/categories/权限控制/"}]},{"title":"PSR","date":"2016-04-02T16:45:00.000Z","path":"程序语言/PHP/php-psr/","text":"PSR PSR 来自 PHP FIG(框架协同工作组) 现有规范 PSR-0/PSR-4 自动加载（2014.10.21起弃用，请使用 PSR-4） PSR-1 基本规范 PSR-2 代码风格 PSR-3 日志接口 PSR-0/PSR-4 自动加载class 指 class,interface,trait和其他相似的结构 PSR-0(弃用)按照目录拼接类名 1234/path/to/src/ VendorFoo/ Bar/ Baz.php # VendorFoo_Bar_Baz PSR-4 类名中下划线转为目录 每个命名空间必须有顶级命名空间（vender） 1234Vendor_Name |- Name_space |- Class |-Name.php # \\Vendor_Name\\Name_space\\Class_Name 自动加载方法 函数 12345678910111213141516&lt;?phpfunction autoload($className)&#123; $className = ltrim($className, &apos;\\\\&apos;); $fileName = &apos;&apos;; $namespace = &apos;&apos;; if ($lastNsPos = strrpos($className, &apos;\\\\&apos;)) &#123; $namespace = substr($className, 0, $lastNsPos); $className = substr($className, $lastNsPos + 1); $fileName = str_replace(&apos;\\\\&apos;, DIRECTORY_SEPARATOR, $namespace) . DIRECTORY_SEPARATOR; &#125; $fileName .= str_replace(&apos;_&apos;, DIRECTORY_SEPARATOR, $className) . &apos;.php&apos;; require $fileName;&#125; 类: SplClassLoader PSR-1 基本代码规范 「文件」源文件必须只使用 &lt;?php 和 &lt;?= 这两种标签。 「文件」源文件中php代码的编码格式必须只使用不带字节顺序标记(BOM)的UTF-8。 「类」类名(class name) 必须使用骆驼式(StudlyCaps)写法。 「类」类(class)中的常量必须只由大写字母和下划线(_)组成。 「类」方法名(method name) 必须使用驼峰式(cameCase)。 「文件」一个源文件建议只用来做声明（类(class)，函数(function)，常量(constant)等）或者只用来做一些引起副作用的操作（例如：输出信息，修改.ini配置等）,但不建议同时做这两件事。 PSR-2 代码风格指南针对 PSR-1 的继承和扩展 「缩进」代码必须使用4个空格来进行缩进，而不是用制表符。 「行」一行代码的长度不建议有硬限制；软限制必须为120个字符，建议每行代码80个字符或者更少；非空行不可有空格；一行不可多于一个语句. 「文件」纯 php 文件 `必须` 省略;`必须`以空行结束123456789- 「命名空间」在`命名空间(namespace)`的声明下面`必须`有一行空行，并且在`导入(use)`的声明下面也`必须`有一行空行。- 「类」`类(class)`的左花括号`必须`放到其声明下面自成一行，右花括号则`必须`放到类主体下面自成一行。- 「方法」`方法(method)`的左花括号`必须`放到其声明下面自成一行，右花括号则`必须`放到方法主体的下一行。- 「属性|方法」所有的`属性(property)`和`方法(method)` `必须`有可见性声明；`抽象(abstract)`和`终结(final)`声明`必须`在可见性声明之前；而`静态(static)`声明`必须`在可见性声明之后。- 「关键字和常量」关键字和常量(true，false，null)`必须`小写- 「控制结构」在控制结构关键字的后面`必须`有一个空格；而`方法(method)`和`函数(function)`的关键字的后面`不可`有空格。- 「控制结构」控制结构的左花括号`必须`跟其放在同一行，右花括号`必须`放在该控制结构代码主体的下一行。- 「控制结构」控制结构的左括号之后`不可`有空格，右括号之前也`不可`有空格。- 「类」`不建议`使用`_`来表明方法和属性的保护性 &lt;?phpnamespace Vendor\\Package; use FooInterface;use BarClass as Bar;use OtherVendor\\OtherPackage\\BazClass; class Foo extends Bar implements FooInterface, Bar{ public function sampleFunction($a, $b = null) { if ($a === $b) { bar(); } elseif ($a &gt; $b) { $foo-&gt;bar($arg1); } else { BazClass::bar($arg2, $arg3); } } final public static function bar() { $foo-&gt;bar( $longArgument, $longerArgument, $muchLongerArgument ); $longArgs_shortVars = function ( $longArgument, $longerArgument, $muchLongerArgument = null ) use ($var1) { // body }; } public function aVeryLongMethodName( ClassTypeHint $arg1, &amp;$arg2, array $arg3 = [] ) { switch ($expr) { case 0: echo &apos;First case, with a break&apos;; break; case 1: echo &apos;Second case, which falls through&apos;; // no break case 2: case 3: case 4: echo &apos;Third case, return instead of break&apos;; return; default: echo &apos;Default case&apos;; break; } while ($expr) { // structure body } do { // structure body; } while ($expr); for ($i = 0; $i &lt; 10; $i++) { // for body } foreach ($iterable as $key =&gt; $value) { // foreach body } try { // try body } catch (FirstExceptionType $e) { // catch body } catch (OtherExceptionType $e) { // catch body } } }``` PSR-3 日志接口主要目标是让类库获得一个Psr\\Log\\LoggerInterface对象并能通过简单通用的方式来写日志 8个等级（debug, info, notice, warning, error, critical, alert, emergency） log 方法(等级，日志)； 等级必须一致，否则必须抛出Psr\\Log\\InvalidArgumentException 日志必须是字符串，或者是可__toString的对象 …. 代码检查工具PHP Mess DetectorPHP项目体检工具，根据你设定的标准（如单一文件代码体积，未使用的参数个数，未使用的方法数）检查PHP代码，超出设定的标准时报警。 PHP Copy Paste Detector顾名思义，检查冗余代码的 PHP Dead Code Detector看名字就知道了，检查从未被调用过的方法 PHP Code SnifferPHPLint持续集成jenkins把上述工具以plugins形式整合起来 xinc+phing跟上述工具集成起来做持续集成后的自动化打包发布 参考资料 PHP PSR代码标准中文版 PHP中有什么好的代码自动检查工具吗","tags":[{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"},{"name":"PSR","slug":"PSR","permalink":"http://yoursite.com/tags/PSR/"}],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://yoursite.com/categories/程序语言/"},{"name":"PHP","slug":"程序语言/PHP","permalink":"http://yoursite.com/categories/程序语言/PHP/"}]},{"title":"UML","date":"2016-02-28T15:22:00.000Z","path":"面向对象/uml/","text":"UML 类图中的关系关联关系用于表示一类对象与另一类对象之间有联系程序中，通常将一个类的对象作为另一个类的成员变量 双向关联 单向关联 自关联 在系统中可能会存在一些类的属性对象类型为该类本身，这种特殊的关联关系称为自关联 多重性关联 表示两个关联对象在数量上的对应关系 表示方式 多重性说明 1..1 表示另一个类的一个对象只与该类的一个对象有关系 0..* 表示另一个类的一个对象与该类的零个或多个对象有关系 1..* 表示另一个类的一个对象与该类的一个或多个对象有关系 0..1 表示另一个类的一个对象没有或只与该类的一个对象有关系 m..n 表示另一个类的一个对象与该类最少m，最多n个对象有关系 (m≤n) 聚合关系 整体（整体对象）和部分（成员对象）可以分开独立存在 这里表示的是：汽车和发动机还有个更形象的比喻：雁群和雁子 程序中，成员对象通常作为构造方法、Setter方法或业务方法的参数注入到整体对象中 组合关系 整体和部分不可分开，有相同的生命周期 这里表示的是：头和嘴还有个更形象的比喻：雁子和翅膀 程序中，通常在整体类的构造方法中直接实例化成员类 依赖关系 表示一个事物「使用」另一个事物时使用依赖关系 程序中，依赖关系体现在某个类的方法使用另一个类的对象作为参数 将一个类的对象作为另一个类中方法的参数 在一个类的方法中将另一个类的对象作为其局部变量 在一个类的方法中调用另一个类的静态方法 泛化关系 泛化关系就是继承关系 接口与实现关系 总结各种关系的强弱顺序： 泛化= 实现&gt; 组合&gt; 聚合&gt; 关联&gt; 依赖 参考资料 深入浅出UML类图（二） 深入浅出UML类图（三） UML类图几种关系的总结","tags":[{"name":"uml","slug":"uml","permalink":"http://yoursite.com/tags/uml/"}],"categories":[{"name":"面向对象","slug":"面向对象","permalink":"http://yoursite.com/categories/面向对象/"}]},{"title":"设计原则","date":"2016-02-21T06:04:00.000Z","path":"面向对象/oo-principle/","text":"前言在面对一个复杂项目的时候，为了提高项目的 扩展性、 复用性、 可维护性，我们往往会进行抽象设计；而这些原则是从许多成功设计方案中总结出的指导性原则，为我们学习设计模式和设计系统提供了很好的帮助。 设计原则 设计原则名称 定义 使用频率 开闭原则(Open-Closed Principle, OCP) 一个软件实体对扩展开放，而对修改关闭 ★★★★★ 里氏代换原则(Liskov Substitution Principle, LSP) 所有引用基类对象的地方能够透明地使用其子类的对象 ★★★★★ 依赖倒转原则(Dependence Inversion Principle, DIP) 抽象不应该依赖于细节，细节应该依赖于抽象 ★★★★★ 单一职责原则(Single Responsibility Principle, SRP) 一个类只负责一个功能领域中的相应职责 ★★★★☆ 合成复用原则(Composite Reuse Principle, CRP) 尽量使用对象组合，而不是继承来达到复用的目的 ★★★★☆ 迪米特法则(Law of Demeter, LoD) 一个软件实体应当尽可能少地与其他实体发生相互作用 ★★★☆☆ 接口隔离原则(Interface Segregation Principle, ISP) 使用多个专门的接口，而不使用单一的总接口 ★★☆☆☆ 开闭原则「软件实体」可以指一个软件模块、一个由多个类组成的局部结构或一个独立的类。 作用 在项目的迭代过程中，出现新的需求或者需求改动时，在不修改现有代码的情况下，就可扩展新的行为。(总目标) 如何做？ 思考增加需求时，如何实现？ 抽象化是开闭原则的关键把 可能会发生变化的地方 （ 要对需求的变更有预见性 ）放到抽象类中，具体的实现在子类中完成。如果需要修改系统的行为，无须对抽象层进行任何改动，只需要增加新的具体类来实现新的业务功能即可。 避免过度使用遵循开闭原则，通常会引入新的抽象层次， 增加代码的复杂度。需要把注意力集中在设计中 最有可能改变的地方 ，然后应用开闭原则。每个地方都采用开闭原则，是一种浪费，也没必要，还会导致代码变得复杂且难以理解。 具体实例 多图表-柱状、饼状[2] 里氏替换原则作用 里氏代换原则是实现开闭原则的重要方式之一；保证整个继承体系不被破坏。 如何做？ 子类不能重新定义父类的方法/属性 子类的所有方法必须在父类中声明，或子类必须实现父类中声明的所有方法 尽量把父类设计为抽象类或者接口，让子类继承父类或实现父接口，并实现在父类中声明的方法。（运行时，子类实例替换父类实例） 具体实例 给不同等级的客户发送邮件[3] 依赖倒转原则作用 依赖倒转原则就是面向对象设计的主要实现机制之一；它是系统抽象化的具体实现。依赖倒置原则可以降低类之间的耦合性，提高系统的稳定性，降低修改程序造成的风险。 如何做？ 要针对接口编程，而不是针对实现编程 传递参数时或在关联关系中，尽量引用层次高的抽象层类；使用 接口和抽象类 进行变量类型声明、参数类型声明等使用继承时，遵循里氏替换原则 针对抽象层编程，而将具体类的对象通过 [依赖注入] 的方式注入到其他对象中;[依赖注入]是指当一个对象要与其他对象发生依赖关系时，通过抽象来注入所依赖的对象；主要有以下三种方式：构造注入、设值注入（Setter注入）、接口注入 具体实例 存储不同格式的文件到数据库[4] 单一职责原则作用 单一职责原则是实现 高内聚、低耦合 的指导方针；提高类的复用性。当一个模块或一个类被设计成只支持一组相关功能时，我们说它具有「高内聚」。 如何做？ 发现类的不同职责并将其分离；一个类应该只有一个引起变化的原因 将不同的职责(变化原因)封装在不同的类中 如果多个职责总是同时发生改变则可将它们封装在同一类中 随着系统的成长，随时查看某个类是否超过一个变化原因, 具体实例 客户关系管理[8] 合成复用原则作用 程序设计中，确定复用是用继承还是组合 如何做？ 要判断使用组合/聚合关系（关联关系）还是继承 组合/聚合 优先考虑；两个类之间是“Has-A”的关系应使用组合或聚合；Has-A 表示某一个角色具有某一项责任 优点： “黑盒”复用，因为被包含对象的内部细节对外是不可见（封装性好） 通过获取具有相同类型的对象引用，可以在运行期间动态地定义（对象的）组合。 缺点： 对象过多，不易理解 继承 两个类之间是“Is-A”关系可使用继承；Is-A 表示一个类是另一个类的”一种” 优点： 有效使用继承会有助于对问题的理解，降低复杂度（严格遵循里氏代换原则） 缺点： 滥用继承反而会增加系统构建和维护的难度以及系统的复杂度 “白盒”复用，因为父类的内部细节对于子类而言通常是可见的。（封装性差） 从基类继承而来的实现是静态的，不可能在运行时发生改变，没有足够的灵活性 继承只能在有限的环境中使用（如类没有声明为不能被继承） 具体实例 数据保存在不同的数据库中[6] 迪米特法则作用降低系统的耦合度，使类与类之间保持松散的耦合关系。 如何做？ 思考软件实体之间的交互；确定只和“朋友”打交道 不要和“陌生人”说话、只与你的直接朋友通信；对于一个对象，其朋友包括以下几类： 当前 对象本身 (this)； 参数对象：以参数形式传入到当前对象方法中的对象； 当前对象的 成员对象；如果当前对象的成员对象是一个集合，那么集合中的元素也都是朋友； 当前对象所 创建的对象 应该尽量减少对象之间的交互 如果两个对象之间不必彼此直接通信，那么这两个对象就不应当发生任何直接的相互作用 如果其中的一个对象需要调用另一个对象的某一个方法的话，可以通过第三者转发这个调用 在类之间的设计上，应当尽量创建松耦合的类，类之间的耦合度越低，就越有利于复用，一个处在松耦合中的类一旦被修改，不会对关联的类造成太大波及 在单个类的结构设计上，每一个类都应当尽量降低其成员变量和成员函数的访问权限，一个类型应当尽量设计成不变类；在对其他类的引用上，一个对象对其他对象的引用应当降到最低 具体实例 一个多操作的窗口[7] 接口隔离原则作用 系统解开耦合，从而容易重构，更改和重新部署 如何做？ 接口仅仅提供客户端需要的行为，客户端不需要的行为则隐藏起来 接口职责要单一将大接口中的方法根据 职责不同 分别放在不同的小接口中，以确保每个接口都承担某一单一角色。 为不同的客户端提供宽窄不同的接口接口应该尽量细化，同时接口中的方法应该尽量少，每个接口中只包含一个客户端（如子模块或业务逻辑类）所需的方法即可。 注意：在使用接口隔离原则时，我们需要注意控制接口的粒度。接口太小会导致系统中接口泛滥，不利于维护；接口太大将违背接口隔离原则，灵活性较差，使用起来很不方便。 一般而言，接口中仅包含为某一类用户定制的方法即可，不应该强迫客户依赖于那些它们不用的方法 具体实例 将文件内容转为不同的数据格式存储[5] 总结 不要过度使用设计模式，一切从简。 设计原则 说明 开闭原则 对扩展开放，对修改关闭 里氏替换原则 不要破坏继承体系 依赖倒置原则 面向接口编程 单一职责原则 类要职责单一 接口隔离原则 接口要精简单一 迪米特法则 软件实体间要降低耦合 合成复用原则 复用时要合理使用继承和组合 设计原则之间的关系，个人总结如下： 参考资料 面向对象设计原则概述 面向对象设计原则之开闭原则 面向对象设计原则之里氏代换原则 面向对象设计原则之依赖倒转原则 面向对象设计原则之接口隔离原则 面向对象设计原则之合成复用原则 面向对象设计原则之迪米特法则 面向对象设计原则之单一职责原则","tags":[{"name":"OO","slug":"OO","permalink":"http://yoursite.com/tags/OO/"}],"categories":[{"name":"面向对象","slug":"面向对象","permalink":"http://yoursite.com/categories/面向对象/"}]},{"title":"Pandoc","date":"2016-01-04T00:00:00.000Z","path":"开源软件简析/pandoc/","text":"Pandoc 简介pandoc 是一个 markdown 文档转换工具，可以把 markdown 转换为诸多的格式，可以定制格式，编写过滤器等。pandoc 支持的格式，可以参考官网：http://www.pandoc.org pandoc 的语法，请看：官网英文 / 中文翻译 pandoc 过滤器编写，请看：https://github.com/jgm/pandoc/wiki/Pandoc-Filters 如何自定义样式Html 样式Html 可以指定 css 文件进行样式修改 方式一：引入 css 文件 1234table&#123;border-collapse:collapse;border:1px solid #CCC;background:#efefef;&#125;table caption&#123;text-align:left; background-color:#fff; line-height:2em; font-size:14px; font-weight:bold; &#125;table th&#123;text-align:left; font-weight:bold;height:26px; line-height:26px; font-size:12px; border:1px solid #CCC;&#125;table td&#123;height:20px; font-size:12px; border:1px solid #CCC;background-color:#fff;&#125; 1$ pandoc -s -c [cssfile] [mdfile] -o [htmlname] 方式二：页面内置样式（写入 header） 123456&lt;style type=\"text/css\"&gt;table&#123;border-collapse:collapse;border:1px solid #CCC;background:#efefef;&#125;table caption&#123;text-align:left; background-color:#fff; line-height:2em; font-size:14px; font-weight:bold; &#125;table th&#123;text-align:left; font-weight:bold;height:26px; line-height:26px; font-size:12px; border:1px solid #CCC;&#125;table td&#123;height:20px; font-size:12px; border:1px solid #CCC;background-color:#fff;&#125;&lt;/style&gt; 1$ pandoc -s -H [cssfile] [mdfile] -o [htmlname] Docx 样式pandoc 可以使用 docx 模板进行渲染（注意：模板是修改样式，而不是内容） 可以把自己修改后的样式保存为「word 模板」 1$ pandoc --reference-docx=[模板路径] [mdfile] -o [docxname] Docx 模板分享自己的 docx 模板（不断更新中） 技术文档docx模板 TeX 模板见 Phodal Huang 毕业设计模板 (百度盘备份) 使用方法：需要保存为*.docx 文件，然后使用 --template 选项指定模板 生成 Html 幻灯片利用 markdown 直接生成 web-based slideshow；可以自定义 css ，足够灵活。 支持那些幻灯片框架？pandoc 包含5种 html 幻灯片框架： DZSlides Slidy S5 Slideous reveal.js 实际上可以使用任何幻灯片框架（比如Google I/O HTML5 slide template），只要让Pandoc在渲染HTML时使用你指定的模板即可. 生成默认模板的幻灯片pandoc 内置 dzslides 框架 1$ pandoc [*.md] -o [*.html] -t dzslides -s 可选配置渐进显示 生成幻灯片时加入 -i 选项，用于控制列表的显示效果（逐条渐入） 两段文字显示之间的人为停顿: ... 强制分割默认是2级标题分割。可以使用-----------------强制分割;也可以使用 --slide-level 选项覆盖默认的 Slide level 代码高亮风格控制代码高亮风格的选项有： –highlight-style pygments –highlight-style kate –highlight-style monochrome –highlight-style espresso –highlight-style haddock –highlight-style tango –highlight-style zenburn 自定义样式这里着重分析下 reveal.js ；为什么？因为只有它有提示板。 安装reveal.js1$ git clone https://github.com/hakimel/reveal.js 生成幻灯片 Html1$ pandoc [*.md] -o [*.html] -t revealjs -s [-V theme=beige] 支持的样式： default：（默认）深灰色背景，白色文字 beige：米色背景，深色文字 sky：天蓝色背景，白色细文字 night：黑色背景，白色粗文字 serif：浅色背景，灰色衬线文字 simple：白色背景，黑色文字 solarized：奶油色背景，深青色文字 提示板按 s 触发； 增加小抄： 123456&lt;aside class=\"notes\"&gt; * 这里是提示1 * 这里是提示2&lt;/aside&gt; 参考资料 Markdown+Pandoc→HTML幻灯片速成","tags":[{"name":"pandoc","slug":"pandoc","permalink":"http://yoursite.com/tags/pandoc/"},{"name":"ppt","slug":"ppt","permalink":"http://yoursite.com/tags/ppt/"},{"name":"doc","slug":"doc","permalink":"http://yoursite.com/tags/doc/"},{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"}],"categories":[{"name":"开源软件简析","slug":"开源软件简析","permalink":"http://yoursite.com/categories/开源软件简析/"}]},{"title":"GitFlow","date":"2016-01-02T09:06:00.000Z","path":"GIT/git-flow/","text":"目前收集到的工作流有以下几种，其中最常用的是 Gitflow 和 Forking + Pull request Git工作流集中式 svn 形式的代码管理 所有的代码都在 master 分支上开发（线性开发） 功能分支 以集中式工作流为基础，不同的是新功能在新分支上开发 开发完毕发起 pull request 讨论通过后合并到 master Gitflow 来源2010 年初，荷兰的程序员 Vincent Driessen 在他自己的博客 http://nvie.com/ 发表了一篇文章 《A successful Git branching model》 简述 在功能分支的基础上，增加了维护和开发的便利性 两个长期分支：master 和 develop；三个短期分支：feature，release，hotfix 5个分支说明 master 分支只做发布 develop 分支做开发功能集成 feature 分支来自 develop 分支，用于新特性开发，被合并到 develop 分支 release 分支来自 develop 分支，用于预发布与测试，被合并到 master 分支和 develop 分支 hotfix 分支来自 master 分支，用于修改线上bug，被合并回 master 分支 和 develop 分支/release 分支 插件123456git flow feature start // 开始一个特性的开发git flow feature finish // 完成一个特性的开发git flow release start // 开始一次 releasegit flow release finish // 完成一次 releasegit flow hotfix start // 开始一个线上bug修复git flow hotfix finish // 完成一个线上bug修复 详情见 git flow Forking + Pull request 利用分支合并，方便接受其他贡献者的提交，而无须开放项目权限 贡献者 push 自己的代码到自己的服务端仓库，发起 pull request 项目的维护者review 后合并 github，osc 等都使用此种工作流 本地添加远程项目1git remote add [upstream-name] [upstream-url] pull request的注意事项 base fork 与 head fork 的区别 base fork : 请求 pull requeat 的分支，通常是被 fork 的分支（维护者）； head fork ：希望被合并的分支，通常是自己fork 的分支（贡献者） fork 的项目和原有项目保持同步 方式1：分步处理 需要预览和对比，则使用： 123$ git remote update [upstream-name]$ git checkout [branch-name]$ git rebase [upstream/branch-name] ​方式2：一步处理 不需要预览则使用： 1$ git pull --rebase [upstream-name] [branch-name] 方式3：脚本处理 123456789101112131415161718192021222324252627#!/bin/bashgup() &#123; local br br=\\`git branch 2&gt; /dev/null|\\\\grep &apos;^*&apos;|sed -e &apos;s/..//;s/\\\\n//&apos;\\` tainted=\\`git status --porcelain | \\\\grep -v &apos;^\\\\?\\\\?&apos;\\` if [[ $br == master ]]; then if [[ $tainted == &apos;&apos; ]]; then echo git stash git stash fi echo git fetch git fetch echo git rebase FETCH_HEAD $br git rebase FETCH_HEAD $br else if [[ -n $br ]]; then if [[ $tainted == &apos;&apos; ]]; then echo git stash git stash fi echo git pull --rebase origin $br git pull --rebase origin $br else echo seems not in any branch fi fi&#125; 方式4：使用 github 等自带的页面处理 base fork 选择自己的 fork 分支（贡献者）； head fork 选择被 fork 的分支（维护者）；然后发起 pull request 使用 merge 还是 rebase？ 开发以 pull request + review 为主的模式，merge 最合适 第一次合并/单人开发/同步上游改动时，rebase 最合适 参考资料 一个成功的 Git 分支模型 git-flow 备忘清单 Git Workflows and Tutorials git 里 push request 注意事项","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}],"categories":[{"name":"GIT","slug":"GIT","permalink":"http://yoursite.com/categories/GIT/"}]},{"title":"Web Servive","date":"2015-12-24T16:05:00.000Z","path":"架构设计/web-servive/","text":"WhatWeb服务是一种服务导向架构的技术，通过标准的Web协议提供服务，目的是保证不同平台的应用服务可以互操作。 Why以下观点来自维基百科： 重复使用的应用程序组件 连接现有的软件，在不同的应用程序和平台之间交换数据 How有三种普遍实现方式： 远程过程调用（RPC） 面向动作；目前有很多开源的 RPC 框架：YAR、GRPC、Thrift 等 RPC式WEB服务实质上是利用一个简单的映射，以把用户请求直接转化成为一个特定语言编写的函数或方法. 服务导向架构（SOA[P]） 面向消息； 遵从服务导向架构（Service-oriented architecture，SOA）概念来构筑WEB服务，在服务导向架构中，通讯由消息驱动，而不再是某个动作（方法调用）。 作为与RPC方式的最大差别，SOA方式更加关注如何去连接服务而不是去特定某个实现的细节。 通常使用 SOAP 风格；由 UDDI、 SOAP 和 WSDL 构成；他们之间的关系如下图： SOAP一个基于XML的可扩展消息信封格式，需同时绑定一个网络传输协议。这个协议通常是HTTP或HTTPS，但也可能是SMTP或XMPP。 WSDL一个XML格式文档，用以描述服务端口访问方式和使用协议的细节。通常用来辅助生成服务器和客户端代码及配置信息。 UDDI一个用来发布和搜索WEB服务的协议，应用程序可借由此协议在设计或运行时找到目标WEB服务。 表述性状态转移（REST） 面向资源； 表述性状态转移式（Representational state transfer，REST），是一种架构风格，把接口限定在一组广为人知的标准动作中（比如HTTP的GET、POST、PUT、DELETE）以供调用。 REST 的 基本特征： 客户端和服务器结构 连接协议具有无状态性 能够利用Cache机制增进性能 层次化的系统 REST 的三要素： 唯一的资源标识 简单的方法 一定的表达方式 三要素关系图： REST 是以 资源 为中心, 名词 即资源的地址, 动词 即施加于名词上的一些有限操作, 表达 是对各种资源形态的抽象.以HTTP为例, 名词即为URI(统一资源标识), 动词包括POST, GET, PUT, DELETE等(还有其它不常用的2个,所以 整个动词集合是有限的), 资源的形态(如text, html, image, pdf等) 参考资料 Web服务-维基百科 Web services-维基百科 面向服务的体系结构 REST 远程过程调用 Webservice学习笔记五，Web Service实践之REST vs RPC Webservice学习笔记六，SOAP, REST and XML-RPC报文格式收集 rest-vs-soap Restful-User-Experience","tags":[{"name":"web-service","slug":"web-service","permalink":"http://yoursite.com/tags/web-service/"},{"name":"soa","slug":"soa","permalink":"http://yoursite.com/tags/soa/"},{"name":"rpc","slug":"rpc","permalink":"http://yoursite.com/tags/rpc/"}],"categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://yoursite.com/categories/架构设计/"}]},{"title":"ActiveMQ","date":"2015-11-07T07:55:00.000Z","path":"中间件/MQ/activemq/","text":"什么是 activeMQ？一个开源消息队列,使用 java 开发,属于 apache 基金会.官方网站:http://activemq.apache.org官方文档:http://activemq.apache.org/getting-started.html php 如何和 activeMQ 通信？采用 stomp 协议php 安装 stomp 的扩展 1pecl install stomp 采用第三方的类库:https://github.com/dejanb/stomp-php php 如何监控 activeMQ 中队列的状态？activeMQ 需要打开StatisticsPlugin方法:http://activemq.apache.org/statisticsplugin.html 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 获取指定队列的待处理消息数量 * * @param string $queue 待查询的队列名 * * @return mixed 成功返回待处理的消息数量 */function getMQStatus($queue)&#123; $result = $num = FALSE; $statusqueue = &quot;/queue/ActiveMQ.Statistics.Destination.&#123;$queue&#125;&quot;;//固定格式 //$statusqueue = &quot;/queue/ActiveMQ/Statistics/Destination/&#123;$queue&#125;&quot;;//固定格式 //开启ActiveMQ $link = stomp_connect(&apos;tcp://192.168.221.129:6161&apos;);//broker 的地址 if (!$link) &#123; die(&quot;Can&apos;t connect MQ !!&quot;); &#125; if (FALSE !== $link) &#123; //查询之后的结果存放处 $resultqueue = &quot;/queue/test_status_&#123;$queue&#125;&quot;; //设定采用JSON格式 stomp_subscribe($link, $resultqueue, array(&quot;transformation&quot; =&gt; &quot;jms-map-json&quot;)); //送出空字符串 $result = stomp_send($link, $statusqueue, &apos;&apos;, array(&quot;reply-to&quot; =&gt; $resultqueue)); if (FALSE === $result) &#123; echo &quot; send error&quot; . PHP_EOL; &#125; //取得状态 while (stomp_has_frame($link)) &#123; $frame = stomp_read_frame($link); if (FALSE != $frame) &#123; stomp_ack($link, $frame[&apos;headers&apos;][&apos;message-id&apos;]); $obj = json_decode($frame[&apos;body&apos;], TRUE);//$obj 包含队列的所有信息 //print_r($obj); //取得目前数量（尚可取得其他状态） foreach ($obj[&apos;map&apos;][&apos;entry&apos;] as $pitem) &#123; if (&apos;size&apos; == $pitem[&apos;string&apos;]) &#123; $num = $pitem[&apos;long&apos;]; break; &#125; &#125; &#125; &#125; stomp_unsubscribe($link, $resultqueue); stomp_close($link); &#125; return $num;&#125;$num = getMQStatus(&apos;spider&apos;);","tags":[{"name":"php","slug":"php","permalink":"http://yoursite.com/tags/php/"},{"name":"MQ","slug":"MQ","permalink":"http://yoursite.com/tags/MQ/"},{"name":"activeMQ","slug":"activeMQ","permalink":"http://yoursite.com/tags/activeMQ/"}],"categories":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/categories/中间件/"},{"name":"MQ","slug":"中间件/MQ","permalink":"http://yoursite.com/categories/中间件/MQ/"}]}]}